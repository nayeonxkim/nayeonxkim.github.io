---
layout: post
title: "타이타닉 분류 모델"
permalink: categories/
---

## 전처리 과정
#### 1. Null 값 처리
#### 2. 분석에 필요없는 칼럼 제거
#### 3. EDA: 변수별 분포와 Survived 비율 확인
#### 4. Ticket 변수 의미 확인 (교차분석)
#### 5. 레이블 인코딩


```python
pip install missingno
```

    Defaulting to user installation because normal site-packages is not writeable
    Requirement already satisfied: missingno in /home/kny0628m/.local/lib/python3.6/site-packages (0.5.1)
    Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from missingno) (3.3.4)
    Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from missingno) (0.11.1)
    Requirement already satisfied: scipy in /home/kny0628m/.local/lib/python3.6/site-packages (from missingno) (1.4.1)
    Requirement already satisfied: numpy in /home/kny0628m/.local/lib/python3.6/site-packages (from missingno) (1.19.5)
    Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->missingno) (8.1.1)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->missingno) (2.4.7)
    Requirement already satisfied: python-dateutil>=2.1 in /home/kny0628m/.local/lib/python3.6/site-packages (from matplotlib->missingno) (2.8.1)
    Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->missingno) (1.3.1)
    Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->missingno) (0.10.0)
    Requirement already satisfied: six in /home/kny0628m/.local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->missingno) (1.16.0)
    Requirement already satisfied: pandas>=0.23 in /home/kny0628m/.local/lib/python3.6/site-packages (from seaborn->missingno) (1.1.5)
    Requirement already satisfied: pytz>=2017.2 in /home/kny0628m/.local/lib/python3.6/site-packages (from pandas>=0.23->seaborn->missingno) (2021.1)
    [33mWARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.
    You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.[0m
    Note: you may need to restart the kernel to use updated packages.


## 1. 패키지 불러오기


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno 

import warnings
warnings.filterwarnings('ignore')

from scipy.stats import chi2_contingency
from scipy import stats
%matplotlib inline

#학습/테스트 데이터 분리
from sklearn.model_selection import train_test_split

#분류 알고리즘
from xgboost import XGBClassifier 

#모델 평가지표
from sklearn.metrics import confusion_matrix, accuracy_score,precision_score, recall_score,f1_score, roc_auc_score

from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, GridSearchCV, KFold
```

## 2. 데이터 불러오기


```python
titanic_df=pd.read_csv('~/data/titanic_train.csv')
titanic_df.head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




```python
titanic_df.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 891 entries, 0 to 890
    Data columns (total 12 columns):
     #   Column       Non-Null Count  Dtype  
    ---  ------       --------------  -----  
     0   PassengerId  891 non-null    int64  
     1   Survived     891 non-null    int64  
     2   Pclass       891 non-null    int64  
     3   Name         891 non-null    object 
     4   Sex          891 non-null    object 
     5   Age          714 non-null    float64
     6   SibSp        891 non-null    int64  
     7   Parch        891 non-null    int64  
     8   Ticket       891 non-null    object 
     9   Fare         891 non-null    float64
     10  Cabin        204 non-null    object 
     11  Embarked     889 non-null    object 
    dtypes: float64(2), int64(5), object(5)
    memory usage: 83.7+ KB



```python
titanic_df.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>714.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
      <td>891.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>446.000000</td>
      <td>0.383838</td>
      <td>2.308642</td>
      <td>29.699118</td>
      <td>0.523008</td>
      <td>0.381594</td>
      <td>32.204208</td>
    </tr>
    <tr>
      <th>std</th>
      <td>257.353842</td>
      <td>0.486592</td>
      <td>0.836071</td>
      <td>14.526497</td>
      <td>1.102743</td>
      <td>0.806057</td>
      <td>49.693429</td>
    </tr>
    <tr>
      <th>min</th>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.420000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>223.500000</td>
      <td>0.000000</td>
      <td>2.000000</td>
      <td>20.125000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.910400</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>446.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>28.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>14.454200</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>668.500000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>38.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>31.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>891.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>80.000000</td>
      <td>8.000000</td>
      <td>6.000000</td>
      <td>512.329200</td>
    </tr>
  </tbody>
</table>
</div>



## 3. EDA 및 데이터 전처리
### 3-1. Null값 처리

#### 결측치 확인


```python
msno.matrix(titanic_df,figsize=(12,5))
```




    <AxesSubplot:>




    
![png](output_10_1.png)
    



```python
#전체 데이터에서 결측치의 비율 확인
titanic_df.isnull().sum() / len(titanic_df)*100
```




    PassengerId     0.000000
    Survived        0.000000
    Pclass          0.000000
    Name            0.000000
    Sex             0.000000
    Age            19.865320
    SibSp           0.000000
    Parch           0.000000
    Ticket          0.000000
    Fare            0.000000
    Cabin          77.104377
    Embarked        0.224467
    dtype: float64



- Cabin: 77%가 결측치이므로 Cabin 칼럼 전체를 제거한다.
- Embarked: 결측치가 아주 적으며, 카테고리형이므로 최빈값으로 대체한다.
- Age: 20%가 결측치, 데이터 수가 적으므로 제거하지 않고 대체한다.

#### 결측치 처리: Cabin, Embarked, Age


```python
#Cabin과 Embarked의 결측치 처리
titanic_df.drop(['Cabin'],axis=1,inplace=True)
titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0],inplace=True)
```

Age의 결측치 처리
- Name의 호칭: 성별과 나이를 어느정도 알 수 있음
- Name칼럼을 이용해 Age 결측치 처리


```python
#Name에서 호칭만 분리하여 Title 변수 생성
titanic_df['Title']=""

for i in titanic_df:
    titanic_df['Title']=titanic_df['Name'].str.extract('([A-Za-z]+)\.')
```


```python
pd.crosstab(titanic_df['Title'],titanic_df['Sex']).T.style.background_gradient()
```




<style  type="text/css" >
#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col0,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col1,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col3,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col4,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col5,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col7,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col8,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col12,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col15,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col16,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col2,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col6,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col9,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col10,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col11,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col13,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col14{
            background-color:  #fff7fb;
            color:  #000000;
        }#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col2,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col6,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col9,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col10,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col11,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col13,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col14,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col0,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col1,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col3,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col4,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col5,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col7,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col8,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col12,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col15,#T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col16{
            background-color:  #023858;
            color:  #f1f1f1;
        }</style><table id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990" ><thead>    <tr>        <th class="index_name level0" >Title</th>        <th class="col_heading level0 col0" >Capt</th>        <th class="col_heading level0 col1" >Col</th>        <th class="col_heading level0 col2" >Countess</th>        <th class="col_heading level0 col3" >Don</th>        <th class="col_heading level0 col4" >Dr</th>        <th class="col_heading level0 col5" >Jonkheer</th>        <th class="col_heading level0 col6" >Lady</th>        <th class="col_heading level0 col7" >Major</th>        <th class="col_heading level0 col8" >Master</th>        <th class="col_heading level0 col9" >Miss</th>        <th class="col_heading level0 col10" >Mlle</th>        <th class="col_heading level0 col11" >Mme</th>        <th class="col_heading level0 col12" >Mr</th>        <th class="col_heading level0 col13" >Mrs</th>        <th class="col_heading level0 col14" >Ms</th>        <th class="col_heading level0 col15" >Rev</th>        <th class="col_heading level0 col16" >Sir</th>    </tr>    <tr>        <th class="index_name level0" >Sex</th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>    </tr></thead><tbody>
                <tr>
                        <th id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990level0_row0" class="row_heading level0 row0" >female</th>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col0" class="data row0 col0" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col1" class="data row0 col1" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col2" class="data row0 col2" >1</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col3" class="data row0 col3" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col4" class="data row0 col4" >1</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col5" class="data row0 col5" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col6" class="data row0 col6" >1</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col7" class="data row0 col7" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col8" class="data row0 col8" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col9" class="data row0 col9" >182</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col10" class="data row0 col10" >2</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col11" class="data row0 col11" >1</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col12" class="data row0 col12" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col13" class="data row0 col13" >125</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col14" class="data row0 col14" >1</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col15" class="data row0 col15" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row0_col16" class="data row0 col16" >0</td>
            </tr>
            <tr>
                        <th id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990level0_row1" class="row_heading level0 row1" >male</th>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col0" class="data row1 col0" >1</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col1" class="data row1 col1" >2</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col2" class="data row1 col2" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col3" class="data row1 col3" >1</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col4" class="data row1 col4" >6</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col5" class="data row1 col5" >1</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col6" class="data row1 col6" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col7" class="data row1 col7" >2</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col8" class="data row1 col8" >40</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col9" class="data row1 col9" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col10" class="data row1 col10" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col11" class="data row1 col11" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col12" class="data row1 col12" >517</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col13" class="data row1 col13" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col14" class="data row1 col14" >0</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col15" class="data row1 col15" >6</td>
                        <td id="T_8473903a_e1cc_11ec_ba68_ac1f6bb08990row1_col16" class="data row1 col16" >1</td>
            </tr>
    </tbody></table>



- Master, Miss, Mr, Mrs가 대부분이고 나머지 호칭들은 적음


```python
#유사한 호칭을 카테고리로 묶음
for i in range(len(titanic_df['Title'])):
    titanic_df['Title'] =  titanic_df['Title'].replace(['Lady', 'Ms','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Others')

    titanic_df['Title'] =  titanic_df['Title'].replace('Mlle', 'Miss') #마드모아젤
    titanic_df['Title']=  titanic_df['Title'].replace('Countess', 'Mrs') #백작부인
    titanic_df['Title'] =  titanic_df['Title'].replace('Mme', 'Mrs') #마담
```


```python
titanic_df['Title'].value_counts()
```




    Mr        517
    Miss      184
    Mrs       127
    Master     40
    Others     23
    Name: Title, dtype: int64




```python
#호칭별 평균나이
titanic_df.groupby('Title')['Age'].mean().round(1)
```




    Title
    Master     4.6
    Miss      21.8
    Mr        32.4
    Mrs       35.8
    Others    45.3
    Name: Age, dtype: float64



- 호칭별로 나이에서 유의미한 차이가 존재한다.
- 같은 성별인데도 여자는 14살, 남자는 28살 차이가 난다.


```python
#호칭별 평균 나이로 Age의 Null값 대체
titanic_df["Age"].fillna(titanic_df.groupby(['Title'])["Age"].transform('mean').round(2) , inplace = True)
```


```python
#결측치 없어졌는지 확인
titanic_df.isnull().sum()
```




    PassengerId    0
    Survived       0
    Pclass         0
    Name           0
    Sex            0
    Age            0
    SibSp          0
    Parch          0
    Ticket         0
    Fare           0
    Embarked       0
    Title          0
    dtype: int64



### 3-2. 필요없는 칼럼 제거


```python
titanic_df.drop(['PassengerId','Name'],axis=1,inplace=True)
```

### 3-3. 변수 둘러보기 (EDA)


```python
#Target 변수: Survived
plt.figure(figsize=(8,10))
titanic_df['Survived'].value_counts().plot.pie(autopct='%.1f%%',cmap='Set3',startangle=90,table=True)
plt.title('Survived')
plt.show()
```


    
![png](output_28_0.png)
    


- y값이 한 쪽에 심하게 치우치지 않았다.
- 사망자가 61.6%로 생존자보다 더 많다.


```python
#Fare
plt.figure(figsize=(6,10))
sns.boxplot(x='Pclass',y='Fare',data=titanic_df)
plt.title('Fare Boxplot')
plt.show()
```


    
![png](output_30_0.png)
    


- Pclass가 작을수록 Fare가 큼: 보통의 경우처럼 Pclass 1이 가장 높은 등급, Pclass 3이 가장 낮은 등급인 것을 확인함
- Pclass1에서 500달러 이상의 Outlier가 발견됨: 데이터 수가 많지 않고 500달러 이상을 주고 살 수도 있다고 생각하여 제거X


```python
#Pclass
f,ax = plt.subplots(1,2,figsize=(15,5))

titanic_df['Pclass'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True,cmap='Set3')
ax[0].set_title('Pclass')
ax[0].set_ylabel('')

titanic_df.groupby('Pclass')['Survived'].value_counts(normalize=True).unstack('Survived').plot.bar(stacked=True, color=['grey', 'green'], ax=ax[1])
ax[1].set_title('Pclass-Survived')
plt.show()
```


    
![png](output_32_0.png)
    


- Pclass3에 탑승자가 가장 많다.
- 클래스 별로 인원이 다름: 단순 count로 Survived로 확인하는 것은 비효율적
- 각 클래스의 총 인원을 100으로 보았을 때, 생존자와 사망자의 비율
- "객실 등급이 낮을수록 사망자의 비율이 높다."


```python
#Sex
f,ax = plt.subplots(1,2,figsize=(15,5))

titanic_df['Sex'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True,cmap='Set3')
ax[0].set_title('Sex')
ax[0].set_ylabel('')

titanic_df.groupby('Sex')['Survived'].value_counts(normalize=True).unstack('Survived').plot.bar(stacked=True, color=['grey', 'green'], ax=ax[1])
ax[1].set_title('Sex-Survived')
plt.show()
```


    
![png](output_34_0.png)
    


- 남자 탑승자가 더 많다.
- "여자는 생존자가 과반수, 남자는 사망자가 과반수"


```python
#Age
def get_category(age):
    cat = ''
    if age <= 9: cat = 'Child'
    elif age <= 19: cat = 'Teenager'
    elif age <= 29: cat = 'Young'
    elif age <= 39: cat = 'Adult'
    elif age <= 59: cat ='Milddle'
    else : cat = 'Elderly'
    
    return cat

titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))
age_names = ['Child','Teenager','Young','Adult','Middle','Elderly']

f,ax = plt.subplots(1,2,figsize=(15,5))

titanic_df['Age_cat'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True,cmap='Set3')
ax[0].set_title('Age')
ax[0].set_ylabel('')

titanic_df.groupby('Age_cat')['Survived'].value_counts(normalize=True).unstack('Survived').plot.bar(stacked=True, color=['grey', 'green'], ax=ax[1])
ax[1].set_title('Age-Survived')
plt.show()

titanic_df.drop(['Age_cat'],axis=1,inplace=True)
```


    
![png](output_36_0.png)
    


- Age 연속형: 보기 쉽게 카테고리로 묶음
- 20, 30대 탑승자(Young, Adult)가 가장 많다.
- Child의 생존율이 가장 높다.
- 나이가 어릴수록 생존율이 높다. (but 차이 크지 않음)


```python
#SibSp, Parch
f,ax = plt.subplots(1,2,figsize=(15,5))

sns.countplot(y='SibSp',data=titanic_df,ax=ax[0],palette='Set2')
ax[0].set_title('SibSp')

sns.countplot(y='Parch',data=titanic_df,ax=ax[1],palette='Set2')
ax[1].set_title('Parch')
plt.show()
```


    
![png](output_38_0.png)
    


- SibSp: 동반한 형제자매/배우자의 수
- Parch: 동반한 부모/자녀의 수
- 두 변수 모두 동반한 사람이 없는 경우가 가장 많다.
- 두 변수 모두 동반한 사람이 3명 이상인 경우는 매우 적다.


```python
#Embarked
f,ax = plt.subplots(1,2,figsize=(15,5))

titanic_df['Embarked'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True,cmap='Set3')
ax[0].set_title('Embarked')
ax[0].set_ylabel('')

titanic_df.groupby('Embarked')['Survived'].value_counts(normalize=True).unstack('Survived').plot.bar(stacked=True, color=['grey', 'green'], ax=ax[1])
ax[1].set_title('Embarked-Survived')
plt.show()
```


    
![png](output_40_0.png)
    


- S(사우샘프턴)에서 탑승한 사람이 가장 많다.
- C(셰르부르) 탑승객의 생존율이 가장 높다.


```python
#Title
f,ax = plt.subplots(1,2,figsize=(15,5))

titanic_df['Title'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True,cmap='Set3')
ax[0].set_title('Title')
ax[0].set_ylabel('')

titanic_df.groupby('Title')['Survived'].value_counts(normalize=True).unstack('Survived').plot.bar(stacked=True, color=['grey', 'green'], ax=ax[1])
ax[1].set_title('Title-Survived')
plt.show()
```


    
![png](output_42_0.png)
    


- 기혼 남성 승객이 가장 많다.
- But Mr의 사망율이 가장 높다. 
- Mrs와 Miss/ Master는 상대적으로 생존율이 높다: 여성/어린이라서 우선적으로 구조되었을 것


```python
#Ticket
titanic_df['Ticket'].value_counts()
```




    347082      7
    1601        7
    CA. 2343    7
    CA 2144     6
    3101295     6
               ..
    13049       1
    349242      1
    237442      1
    34218       1
    347083      1
    Name: Ticket, Length: 681, dtype: int64



- 일련번호 형식(코드같은 느낌): count해서 같은 값이 있는지 확인
- 같은 코드=같은 티켓
- 같은 코드의 티켓을 소지한 승객들은 동반했다고 해석

### 3-4. Ticket 변수의 의미 확인하기
- 'Ticket으로 동반탑승한 사람의 수를 확인할 수 있다' 해석
- SibSp와 Parch가 동반한 가족을 나타내는 변수이므로 Ticket과 두 변수의 관련성이 높다면 위 해석이 맞는 것으로 간주  : Ticket 제거

### 교차분석
상관분석: 두 변수 간의 선형관계의 방향과 강도를 확인한다. = 선형성을 기본 가정으로 함

but 범주형 변수는 선형관계로 나타낼 수 없다.

두 범주형 변수 간의 관련성을 확인할 때 사용하는 방법이 교차분석(카이제곱 검정)이다.

- 귀무가설1: SibSp와 Ticket은 독립사건이다. (관련성X)
- 대립가설1: SibSp와 Ticket은 종속사건이다. (관련성o)



- 귀무가설2: Parch와 Ticket은 독립사건이다. (관련성X)
- 대립가설2: Parch와 Ticket은 종속사건이다. (관련성o)


```python
#같은 코드를 가진 수대로 Ticket 카테고리화
#1개: single, 2개: Double, 3개 이상: Multiple

titanic_df['Ticket_cat']=""

for i in range(len(titanic_df['Ticket'])):
    if titanic_df['Ticket'][i] in (titanic_df['Ticket'].value_counts()[titanic_df['Ticket'].value_counts()==1]).index:
        titanic_df['Ticket_cat'][i]='Single'
    elif titanic_df['Ticket'][i] in (titanic_df['Ticket'].value_counts()[titanic_df['Ticket'].value_counts()==2]).index:
        titanic_df['Ticket_cat'][i]='Double'
    else:
        titanic_df['Ticket_cat'][i]='Multiple'

sns.countplot(y='Ticket_cat',data=titanic_df,palette='Set2')
plt.show()
```


    
![png](output_48_0.png)
    


- 동반 탑승객이 없이 혼자 탑승한 승객이 많다.


```python
#교차분석을 위해 SibSp, Parch도 카테고리화
titanic_df['SibSp_cat']=""

for i in range(len(titanic_df['SibSp'])):
    if titanic_df['SibSp'][i]==0:
        titanic_df['SibSp_cat'][i]='Single'
    elif titanic_df['SibSp'][i] ==1:
        titanic_df['SibSp_cat'][i]='Double'
    else:
        titanic_df['SibSp_cat'][i]='Multiple'
        
titanic_df['Parch_cat']=""

for i in range(len(titanic_df['Parch'])):
    if titanic_df['Parch'][i]==0:
        titanic_df['Parch_cat'][i]='Single'
    elif titanic_df['Parch'][i] ==1:
        titanic_df['Parch_cat'][i]='Double'
    else:
        titanic_df['Parch_cat'][i]='Multiple'
```


```python
f,ax= plt.subplots(1,2,figsize=(15,5))

group_names = ['Single','Double','Multiple']

sns.countplot(y='SibSp_cat',data=titanic_df,ax=ax[0],palette='Set2',order=group_names)
ax[0].set_title('Sibling&Spouse Category')


sns.countplot(y='Parch_cat',data=titanic_df,ax=ax[1],palette='Set2')
ax[1].set_title('Parent&Child Category')

plt.show()
```


    
![png](output_51_0.png)
    



```python
#Ticket과 SibSp, Parch의 상호연관성 확인: 교차분석
#Ticket과 SibSp 교차표
titanic_df['Ticket_cat'] = titanic_df['Ticket_cat'].astype('category')
titanic_df['Ticket_cat'] =titanic_df['Ticket_cat'].cat.set_categories(['Single', 'Double', 'Multiple'], ordered=True)

titanic_df['SibSp_cat'] = titanic_df['SibSp_cat'].astype('category')
titanic_df['SibSp_cat'] =titanic_df['SibSp_cat'].cat.set_categories(['Single', 'Double', 'Multiple'], ordered=True)

s_cross=pd.crosstab(titanic_df['Ticket_cat'],titanic_df['SibSp_cat'])

#Ticket과 Parch 교차표
titanic_df['Parch_cat'] = titanic_df['Parch_cat'].astype('category')
titanic_df['Parch_cat'] =titanic_df['Parch_cat'].cat.set_categories(['Single', 'Double', 'Multiple'], ordered=True)

p_cross=pd.crosstab(titanic_df['Ticket_cat'],titanic_df['Parch_cat'])
```


```python
#교차표가 뭔지
s_cross.T.style
```




<style  type="text/css" >
</style><table id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990" ><thead>    <tr>        <th class="index_name level0" >Ticket_cat</th>        <th class="col_heading level0 col0" >Single</th>        <th class="col_heading level0 col1" >Double</th>        <th class="col_heading level0 col2" >Multiple</th>    </tr>    <tr>        <th class="index_name level0" >SibSp_cat</th>        <th class="blank" ></th>        <th class="blank" ></th>        <th class="blank" ></th>    </tr></thead><tbody>
                <tr>
                        <th id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990level0_row0" class="row_heading level0 row0" >Single</th>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row0_col0" class="data row0 col0" >482</td>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row0_col1" class="data row0 col1" >71</td>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row0_col2" class="data row0 col2" >55</td>
            </tr>
            <tr>
                        <th id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990level0_row1" class="row_heading level0 row1" >Double</th>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row1_col0" class="data row1 col0" >57</td>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row1_col1" class="data row1 col1" >103</td>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row1_col2" class="data row1 col2" >49</td>
            </tr>
            <tr>
                        <th id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990level0_row2" class="row_heading level0 row2" >Multiple</th>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row2_col0" class="data row2 col0" >8</td>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row2_col1" class="data row2 col1" >14</td>
                        <td id="T_88e03088_e1cc_11ec_ba68_ac1f6bb08990row2_col2" class="data row2 col2" >52</td>
            </tr>
    </tbody></table>




```python
#Ticket&SibSp 교차분석
result=chi2_contingency(observed=s_cross, correction=False)
print("1. 카이제곱 통계량:", result[0])
print("2. p-value:", result[1])
```

    1. 카이제곱 통계량: 353.3017009610004
    2. p-value: 3.3968604361848484e-75



```python
#Ticket&Parch 교차분석
result=chi2_contingency(observed=p_cross, correction=False)
print("1. 카이제곱 통계량:", result[0])
print("2. p-value:", result[1])
```

    1. 카이제곱 통계량: 345.5820567396321
    2. p-value: 1.5770200309296128e-73


- 둘 다 P-value가 0.00으로 통계적으로 유의미하다.
- 귀무가설 기각: Ticket과 SibSp, Parch의 관련성이 있다.
- SibSp와 Parch가 Ticket보다 더 자세한 데이터를 담고 있으므로 Ticket 칼럼을 제거한다.


```python
#TIcket과 교차분석을 위해 생성한 칼럼 제거
titanic_df.drop(['Ticket','Ticket_cat','SibSp_cat','Parch_cat'],axis=1,inplace=True)
```

### 3-5. 레이블 인코딩
- Sex, Title, Embarked


```python
from sklearn import preprocessing

def encode_features(dataDF):
    features = ['Sex','Title', 'Embarked']
    for feature in features:
        le = preprocessing.LabelEncoder()
        le = le.fit(dataDF[feature])
        dataDF[feature] = le.transform(dataDF[feature])
        
    return dataDF

titanic_df = encode_features(titanic_df)
titanic_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>



## 4. XGBoost

### 4-1. 모든 변수로 학습

학습 및 예측


```python
y=titanic_df['Survived']
X=titanic_df.drop(['Survived'],axis=1)

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=156)

xgb_model=XGBClassifier(n_esimators=400,learning_rate=0.1,max_depth=3,random_state=156)

#조기 중단
evals=[(X_test,y_test)]
xgb_model.fit(X_train,y_train,early_stopping_rounds=100,eval_metric='logloss',eval_set=evals,verbose=True)

xgb_preds=xgb_model.predict(X_test)
xgb_pred_proba=xgb_model.predict_proba(X_test)[:,1]
```

    [02:01:52] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64758
    [1]	validation_0-logloss:0.61120
    [2]	validation_0-logloss:0.57935
    [3]	validation_0-logloss:0.55275
    [4]	validation_0-logloss:0.52998
    [5]	validation_0-logloss:0.51097
    [6]	validation_0-logloss:0.49484
    [7]	validation_0-logloss:0.48121
    [8]	validation_0-logloss:0.47012
    [9]	validation_0-logloss:0.46013
    [10]	validation_0-logloss:0.45244
    [11]	validation_0-logloss:0.44581
    [12]	validation_0-logloss:0.44048
    [13]	validation_0-logloss:0.43551
    [14]	validation_0-logloss:0.42998
    [15]	validation_0-logloss:0.42681
    [16]	validation_0-logloss:0.42282
    [17]	validation_0-logloss:0.42029
    [18]	validation_0-logloss:0.41694
    [19]	validation_0-logloss:0.41477
    [20]	validation_0-logloss:0.41243
    [21]	validation_0-logloss:0.41048
    [22]	validation_0-logloss:0.40852
    [23]	validation_0-logloss:0.40855
    [24]	validation_0-logloss:0.40779
    [25]	validation_0-logloss:0.40637
    [26]	validation_0-logloss:0.40503
    [27]	validation_0-logloss:0.40432
    [28]	validation_0-logloss:0.40480
    [29]	validation_0-logloss:0.40424
    [30]	validation_0-logloss:0.40335
    [31]	validation_0-logloss:0.40189
    [32]	validation_0-logloss:0.40105
    [33]	validation_0-logloss:0.40084
    [34]	validation_0-logloss:0.40077
    [35]	validation_0-logloss:0.40115
    [36]	validation_0-logloss:0.40093
    [37]	validation_0-logloss:0.39999
    [38]	validation_0-logloss:0.39959
    [39]	validation_0-logloss:0.39905
    [40]	validation_0-logloss:0.39815
    [41]	validation_0-logloss:0.39826
    [42]	validation_0-logloss:0.39775
    [43]	validation_0-logloss:0.39728
    [44]	validation_0-logloss:0.39710
    [45]	validation_0-logloss:0.39738
    [46]	validation_0-logloss:0.39774
    [47]	validation_0-logloss:0.39819
    [48]	validation_0-logloss:0.39872
    [49]	validation_0-logloss:0.39758
    [50]	validation_0-logloss:0.39770
    [51]	validation_0-logloss:0.39769
    [52]	validation_0-logloss:0.39780
    [53]	validation_0-logloss:0.39696
    [54]	validation_0-logloss:0.39724
    [55]	validation_0-logloss:0.39754
    [56]	validation_0-logloss:0.39741
    [57]	validation_0-logloss:0.39746
    [58]	validation_0-logloss:0.39753
    [59]	validation_0-logloss:0.39782
    [60]	validation_0-logloss:0.39803
    [61]	validation_0-logloss:0.39871
    [62]	validation_0-logloss:0.39881
    [63]	validation_0-logloss:0.39882
    [64]	validation_0-logloss:0.39887
    [65]	validation_0-logloss:0.39928
    [66]	validation_0-logloss:0.39908
    [67]	validation_0-logloss:0.39917
    [68]	validation_0-logloss:0.39993
    [69]	validation_0-logloss:0.39943
    [70]	validation_0-logloss:0.39967
    [71]	validation_0-logloss:0.39969
    [72]	validation_0-logloss:0.39981
    [73]	validation_0-logloss:0.40014
    [74]	validation_0-logloss:0.40047
    [75]	validation_0-logloss:0.40015
    [76]	validation_0-logloss:0.40012
    [77]	validation_0-logloss:0.39998
    [78]	validation_0-logloss:0.40051
    [79]	validation_0-logloss:0.40126
    [80]	validation_0-logloss:0.40044
    [81]	validation_0-logloss:0.40007
    [82]	validation_0-logloss:0.40033
    [83]	validation_0-logloss:0.40029
    [84]	validation_0-logloss:0.40030
    [85]	validation_0-logloss:0.40052
    [86]	validation_0-logloss:0.40025
    [87]	validation_0-logloss:0.40037
    [88]	validation_0-logloss:0.39984
    [89]	validation_0-logloss:0.39892
    [90]	validation_0-logloss:0.39897
    [91]	validation_0-logloss:0.39916
    [92]	validation_0-logloss:0.39868
    [93]	validation_0-logloss:0.39787
    [94]	validation_0-logloss:0.39720
    [95]	validation_0-logloss:0.39687
    [96]	validation_0-logloss:0.39701
    [97]	validation_0-logloss:0.39746
    [98]	validation_0-logloss:0.39767
    [99]	validation_0-logloss:0.39692


평가


```python
#성능평가 지표를 한꺼번에 출력하는 함수 정의
def get_clf_eval(y_test , pred):
    confusion = confusion_matrix( y_test, pred)
    accuracy = accuracy_score(y_test , pred)
    precision = precision_score(y_test , pred)
    recall = recall_score(y_test , pred)
    f1 = f1_score(y_test,pred)
    roc_auc = roc_auc_score(y_test, pred)
    print('오차 행렬')
    print(confusion)
    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))
```


```python
get_clf_eval(y_test,xgb_preds)
```

    오차 행렬
    [[95 12]
     [22 50]]
    정확도: 0.8101, 정밀도: 0.8065, 재현율: 0.6944,    F1: 0.7463, AUC:0.7911



```python
from xgboost import plot_importance
import matplotlib.pyplot as plt
%matplotlib inline

fig,ax=plt.subplots(figsize=(10,7))
plot_importance(xgb_model,ax=ax)
```




    <AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>




    
![png](output_67_1.png)
    


- Parch의 중요도가 다른 변수들에 비해 크게 떨어짐
- Parch를 제외한 변수들로 다시 학습

### 4-2. Parch 제외


```python
X1=titanic_df.drop(['Survived','Parch'],axis=1)
X_train1,X_test1,y_train1,y_test1=train_test_split(X1,y,test_size=0.2,random_state=156)

#모델 학습
xgb_model_par=XGBClassifier(n_esimators=400,learning_rate=0.1,max_depth=3,random_state=156)

#조기 중단
evals1=[(X_test1,y_test1)]
xgb_model_par.fit(X_train1,y_train1,early_stopping_rounds=100,eval_metric='logloss',eval_set=evals1,verbose=True)

xgb_preds_par=xgb_model_par.predict(X_test1)
xgb_pred_proba_par=xgb_model_par.predict_proba(X_test1)[:,1]
```

    [02:01:53] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64758
    [1]	validation_0-logloss:0.61120
    [2]	validation_0-logloss:0.57935
    [3]	validation_0-logloss:0.55275
    [4]	validation_0-logloss:0.52998
    [5]	validation_0-logloss:0.51097
    [6]	validation_0-logloss:0.49484
    [7]	validation_0-logloss:0.48121
    [8]	validation_0-logloss:0.47012
    [9]	validation_0-logloss:0.46013
    [10]	validation_0-logloss:0.45244
    [11]	validation_0-logloss:0.44581
    [12]	validation_0-logloss:0.44048
    [13]	validation_0-logloss:0.43551
    [14]	validation_0-logloss:0.42998
    [15]	validation_0-logloss:0.42681
    [16]	validation_0-logloss:0.42282
    [17]	validation_0-logloss:0.42029
    [18]	validation_0-logloss:0.41694
    [19]	validation_0-logloss:0.41477
    [20]	validation_0-logloss:0.41242
    [21]	validation_0-logloss:0.41048
    [22]	validation_0-logloss:0.40852
    [23]	validation_0-logloss:0.40855
    [24]	validation_0-logloss:0.40779
    [25]	validation_0-logloss:0.40637
    [26]	validation_0-logloss:0.40503
    [27]	validation_0-logloss:0.40432
    [28]	validation_0-logloss:0.40480
    [29]	validation_0-logloss:0.40424
    [30]	validation_0-logloss:0.40335
    [31]	validation_0-logloss:0.40189
    [32]	validation_0-logloss:0.40126
    [33]	validation_0-logloss:0.40104
    [34]	validation_0-logloss:0.40098
    [35]	validation_0-logloss:0.40135
    [36]	validation_0-logloss:0.40113
    [37]	validation_0-logloss:0.40017
    [38]	validation_0-logloss:0.39976
    [39]	validation_0-logloss:0.39924
    [40]	validation_0-logloss:0.39934
    [41]	validation_0-logloss:0.39845
    [42]	validation_0-logloss:0.39794
    [43]	validation_0-logloss:0.39745
    [44]	validation_0-logloss:0.39727
    [45]	validation_0-logloss:0.39766
    [46]	validation_0-logloss:0.39802
    [47]	validation_0-logloss:0.39845
    [48]	validation_0-logloss:0.39809
    [49]	validation_0-logloss:0.39870
    [50]	validation_0-logloss:0.39908
    [51]	validation_0-logloss:0.39895
    [52]	validation_0-logloss:0.39956
    [53]	validation_0-logloss:0.39914
    [54]	validation_0-logloss:0.39932
    [55]	validation_0-logloss:0.39938
    [56]	validation_0-logloss:0.39921
    [57]	validation_0-logloss:0.39970
    [58]	validation_0-logloss:0.39963
    [59]	validation_0-logloss:0.39858
    [60]	validation_0-logloss:0.39855
    [61]	validation_0-logloss:0.39889
    [62]	validation_0-logloss:0.39901
    [63]	validation_0-logloss:0.39854
    [64]	validation_0-logloss:0.39852
    [65]	validation_0-logloss:0.39797
    [66]	validation_0-logloss:0.39772
    [67]	validation_0-logloss:0.39771
    [68]	validation_0-logloss:0.39802
    [69]	validation_0-logloss:0.39740
    [70]	validation_0-logloss:0.39812
    [71]	validation_0-logloss:0.39809
    [72]	validation_0-logloss:0.39779
    [73]	validation_0-logloss:0.39743
    [74]	validation_0-logloss:0.39761
    [75]	validation_0-logloss:0.39757
    [76]	validation_0-logloss:0.39708
    [77]	validation_0-logloss:0.39705
    [78]	validation_0-logloss:0.39690
    [79]	validation_0-logloss:0.39655
    [80]	validation_0-logloss:0.39631
    [81]	validation_0-logloss:0.39601
    [82]	validation_0-logloss:0.39652
    [83]	validation_0-logloss:0.39650
    [84]	validation_0-logloss:0.39631
    [85]	validation_0-logloss:0.39665
    [86]	validation_0-logloss:0.39609
    [87]	validation_0-logloss:0.39683
    [88]	validation_0-logloss:0.39642
    [89]	validation_0-logloss:0.39603
    [90]	validation_0-logloss:0.39612
    [91]	validation_0-logloss:0.39643
    [92]	validation_0-logloss:0.39643
    [93]	validation_0-logloss:0.39652
    [94]	validation_0-logloss:0.39621
    [95]	validation_0-logloss:0.39608
    [96]	validation_0-logloss:0.39635
    [97]	validation_0-logloss:0.39667
    [98]	validation_0-logloss:0.39738
    [99]	validation_0-logloss:0.39736



```python
get_clf_eval(y_test1,xgb_preds_par)
```

    오차 행렬
    [[96 11]
     [19 53]]
    정확도: 0.8324, 정밀도: 0.8281, 재현율: 0.7361,    F1: 0.7794, AUC:0.8167


- Parch 칼럼 제외 후 학습한 모델의 성능이 더 높다.

### 4-3. FamilySize 칼럼 생성 후 학습
- SibSp와 Parch를 FamilySize 칼럼으로 합친 후 학습


```python
#SibSp와 Parch로 가족 수를 나타내는 FamilySize 생성, 자기자신을 포함하기 위해 +1
titanic_df['FamilySize'] =titanic_df['SibSp'] + titanic_df['Parch'] + 1 
```


```python
titanic_df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Embarked</th>
      <th>Title</th>
      <th>FamilySize</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>0</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>2</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
X2=titanic_df.drop(['Survived','SibSp','Parch'],axis=1)
X_train2,X_test2,y_train2,y_test2=train_test_split(X2,y,test_size=0.2,random_state=156)

xgb_model_fam=XGBClassifier(n_esimators=400,learning_rate=0.1,max_depth=3,random_state=156)

#조기 중단
evals2=[(X_test2,y_test2)]
xgb_model_fam.fit(X_train2,y_train2,early_stopping_rounds=100,eval_metric='logloss',eval_set=evals2,verbose=True)

xgb_preds_fam=xgb_model_fam.predict(X_test2)
xgb_pred_proba_fam=xgb_model_fam.predict_proba(X_test2)[:,1]
```

    [02:01:54] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64679
    [1]	validation_0-logloss:0.60974
    [2]	validation_0-logloss:0.57720
    [3]	validation_0-logloss:0.54994
    [4]	validation_0-logloss:0.52660
    [5]	validation_0-logloss:0.50703
    [6]	validation_0-logloss:0.48995
    [7]	validation_0-logloss:0.47637
    [8]	validation_0-logloss:0.46487
    [9]	validation_0-logloss:0.45493
    [10]	validation_0-logloss:0.44783
    [11]	validation_0-logloss:0.44073
    [12]	validation_0-logloss:0.43478
    [13]	validation_0-logloss:0.42984
    [14]	validation_0-logloss:0.42385
    [15]	validation_0-logloss:0.41961
    [16]	validation_0-logloss:0.41520
    [17]	validation_0-logloss:0.41258
    [18]	validation_0-logloss:0.40853
    [19]	validation_0-logloss:0.40637
    [20]	validation_0-logloss:0.40365
    [21]	validation_0-logloss:0.40124
    [22]	validation_0-logloss:0.40004
    [23]	validation_0-logloss:0.39798
    [24]	validation_0-logloss:0.39606
    [25]	validation_0-logloss:0.39521
    [26]	validation_0-logloss:0.39362
    [27]	validation_0-logloss:0.39330
    [28]	validation_0-logloss:0.39299
    [29]	validation_0-logloss:0.39231
    [30]	validation_0-logloss:0.39049
    [31]	validation_0-logloss:0.38992
    [32]	validation_0-logloss:0.38882
    [33]	validation_0-logloss:0.38835
    [34]	validation_0-logloss:0.38799
    [35]	validation_0-logloss:0.38780
    [36]	validation_0-logloss:0.38753
    [37]	validation_0-logloss:0.38769
    [38]	validation_0-logloss:0.38659
    [39]	validation_0-logloss:0.38645
    [40]	validation_0-logloss:0.38611
    [41]	validation_0-logloss:0.38549
    [42]	validation_0-logloss:0.38545
    [43]	validation_0-logloss:0.38544
    [44]	validation_0-logloss:0.38521
    [45]	validation_0-logloss:0.38456
    [46]	validation_0-logloss:0.38423
    [47]	validation_0-logloss:0.38407
    [48]	validation_0-logloss:0.38379
    [49]	validation_0-logloss:0.38288
    [50]	validation_0-logloss:0.38215
    [51]	validation_0-logloss:0.38268
    [52]	validation_0-logloss:0.38193
    [53]	validation_0-logloss:0.38161
    [54]	validation_0-logloss:0.38162
    [55]	validation_0-logloss:0.38173
    [56]	validation_0-logloss:0.38166
    [57]	validation_0-logloss:0.38145
    [58]	validation_0-logloss:0.38134
    [59]	validation_0-logloss:0.38127
    [60]	validation_0-logloss:0.38124
    [61]	validation_0-logloss:0.38114
    [62]	validation_0-logloss:0.38107
    [63]	validation_0-logloss:0.38068
    [64]	validation_0-logloss:0.38065
    [65]	validation_0-logloss:0.38043
    [66]	validation_0-logloss:0.38051
    [67]	validation_0-logloss:0.38094
    [68]	validation_0-logloss:0.38088
    [69]	validation_0-logloss:0.38033
    [70]	validation_0-logloss:0.38027
    [71]	validation_0-logloss:0.38011
    [72]	validation_0-logloss:0.37992
    [73]	validation_0-logloss:0.38020
    [74]	validation_0-logloss:0.38028
    [75]	validation_0-logloss:0.38058
    [76]	validation_0-logloss:0.38010
    [77]	validation_0-logloss:0.38007
    [78]	validation_0-logloss:0.38002
    [79]	validation_0-logloss:0.38030
    [80]	validation_0-logloss:0.37986
    [81]	validation_0-logloss:0.37994
    [82]	validation_0-logloss:0.37963
    [83]	validation_0-logloss:0.38005
    [84]	validation_0-logloss:0.37925
    [85]	validation_0-logloss:0.37953
    [86]	validation_0-logloss:0.37936
    [87]	validation_0-logloss:0.37964
    [88]	validation_0-logloss:0.37972
    [89]	validation_0-logloss:0.37967
    [90]	validation_0-logloss:0.37955
    [91]	validation_0-logloss:0.37981
    [92]	validation_0-logloss:0.38024
    [93]	validation_0-logloss:0.38027
    [94]	validation_0-logloss:0.38022
    [95]	validation_0-logloss:0.37960
    [96]	validation_0-logloss:0.37987
    [97]	validation_0-logloss:0.38016
    [98]	validation_0-logloss:0.38036
    [99]	validation_0-logloss:0.38103



```python
get_clf_eval(y_test2,xgb_preds_fam)
```

    오차 행렬
    [[95 12]
     [22 50]]
    정확도: 0.8101, 정밀도: 0.8065, 재현율: 0.6944,    F1: 0.7463, AUC:0.7911


<모델 성능 비교>


||모든 변수를 사용한 모델|Parch를 제외한 모델|FamilySize 칼럼을 사용한 모델|
|------|:---:|:---:|:---:|
|**정확도**|0.8101|<span style="color:red">0.8324</span>|0.8101
|**정밀도**|0.8065|<span style="color:red">0.8281</span>|0.8065
|**재현율**| 0.6944|<span style="color:red">0.7361</span>| 0.6944
|**F1 score**|0.7463|<span style="color:red">0.7794</span>|0.7463
|**AUC**|0.7911|<span style="color:red">0.8167</span>|0.7911

- 모든 변수를 사용한 모델과 FamilySize 칼럼을 사용한 모델의 성능이 같다.
- FamilySize는 SibSp와 Parch를 단순히 합친 값이기 때문에 두 칼럼을 사용한 것과 결과적으로 차이가 없다.
- Parch를 제외한 모델의 성능이 가장 높았으므로 이 모델로 최적화를 수행한다.

### 4-4. 최적화: GridSearchCV


```python
params_x={'max_depth':[5,7],
        'min_child_weight':[1,3],
        'colsample_bytree':[0.5,0.75]}
gridcv=GridSearchCV(xgb_model_par,param_grid=params_x,cv=3)
gridcv.fit(X_train1,y_train1,early_stopping_rounds=100,eval_metric='logloss',eval_set=evals1,verbose=True)

xgb_preds_p=xgb_model_par.predict(X_test1)
xgb_pred_proba_p2=xgb_model_par.predict_proba(X_test1)[:,1]
```

    [02:01:55] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65375
    [1]	validation_0-logloss:0.63049
    [2]	validation_0-logloss:0.59483
    [3]	validation_0-logloss:0.57418
    [4]	validation_0-logloss:0.55299
    [5]	validation_0-logloss:0.53878
    [6]	validation_0-logloss:0.52193
    [7]	validation_0-logloss:0.50668
    [8]	validation_0-logloss:0.49655
    [9]	validation_0-logloss:0.48296
    [10]	validation_0-logloss:0.47724
    [11]	validation_0-logloss:0.46668
    [12]	validation_0-logloss:0.45605
    [13]	validation_0-logloss:0.45033
    [14]	validation_0-logloss:0.44189
    [15]	validation_0-logloss:0.43952
    [16]	validation_0-logloss:0.43381
    [17]	validation_0-logloss:0.42966
    [18]	validation_0-logloss:0.42718
    [19]	validation_0-logloss:0.42248
    [20]	validation_0-logloss:0.41814
    [21]	validation_0-logloss:0.41817
    [22]	validation_0-logloss:0.41386
    [23]	validation_0-logloss:0.41061
    [24]	validation_0-logloss:0.40810
    [25]	validation_0-logloss:0.40685
    [26]	validation_0-logloss:0.40607
    [27]	validation_0-logloss:0.40614
    [28]	validation_0-logloss:0.40511
    [29]	validation_0-logloss:0.40312
    [30]	validation_0-logloss:0.40125
    [31]	validation_0-logloss:0.40097
    [32]	validation_0-logloss:0.40226
    [33]	validation_0-logloss:0.40053
    [34]	validation_0-logloss:0.40289
    [35]	validation_0-logloss:0.40191
    [36]	validation_0-logloss:0.40217
    [37]	validation_0-logloss:0.40235
    [38]	validation_0-logloss:0.40195
    [39]	validation_0-logloss:0.40300
    [40]	validation_0-logloss:0.40489
    [41]	validation_0-logloss:0.40434
    [42]	validation_0-logloss:0.40573
    [43]	validation_0-logloss:0.40595
    [44]	validation_0-logloss:0.40558
    [45]	validation_0-logloss:0.40619
    [46]	validation_0-logloss:0.40717
    [47]	validation_0-logloss:0.40867
    [48]	validation_0-logloss:0.40893
    [49]	validation_0-logloss:0.40917
    [50]	validation_0-logloss:0.40968
    [51]	validation_0-logloss:0.40895
    [52]	validation_0-logloss:0.40986
    [53]	validation_0-logloss:0.41080
    [54]	validation_0-logloss:0.41099
    [55]	validation_0-logloss:0.41228
    [56]	validation_0-logloss:0.41122
    [57]	validation_0-logloss:0.41044
    [58]	validation_0-logloss:0.41110
    [59]	validation_0-logloss:0.41037
    [60]	validation_0-logloss:0.40970
    [61]	validation_0-logloss:0.40963
    [62]	validation_0-logloss:0.41127
    [63]	validation_0-logloss:0.41147
    [64]	validation_0-logloss:0.41114
    [65]	validation_0-logloss:0.41186
    [66]	validation_0-logloss:0.41266
    [67]	validation_0-logloss:0.41398
    [68]	validation_0-logloss:0.41391
    [69]	validation_0-logloss:0.41346
    [70]	validation_0-logloss:0.41368
    [71]	validation_0-logloss:0.41344
    [72]	validation_0-logloss:0.41439
    [73]	validation_0-logloss:0.41384
    [74]	validation_0-logloss:0.41397
    [75]	validation_0-logloss:0.41449
    [76]	validation_0-logloss:0.41359
    [77]	validation_0-logloss:0.41458
    [78]	validation_0-logloss:0.41481
    [79]	validation_0-logloss:0.41580
    [80]	validation_0-logloss:0.41713
    [81]	validation_0-logloss:0.41768
    [82]	validation_0-logloss:0.41740
    [83]	validation_0-logloss:0.41750
    [84]	validation_0-logloss:0.41815
    [85]	validation_0-logloss:0.41899
    [86]	validation_0-logloss:0.41963
    [87]	validation_0-logloss:0.41974
    [88]	validation_0-logloss:0.41908
    [89]	validation_0-logloss:0.41973
    [90]	validation_0-logloss:0.41923
    [91]	validation_0-logloss:0.41937
    [92]	validation_0-logloss:0.41952
    [93]	validation_0-logloss:0.41968
    [94]	validation_0-logloss:0.42119
    [95]	validation_0-logloss:0.42086
    [96]	validation_0-logloss:0.42268
    [97]	validation_0-logloss:0.42217
    [98]	validation_0-logloss:0.42254
    [99]	validation_0-logloss:0.42183
    [02:01:55] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65453
    [1]	validation_0-logloss:0.63393
    [2]	validation_0-logloss:0.59799
    [3]	validation_0-logloss:0.57729
    [4]	validation_0-logloss:0.55661
    [5]	validation_0-logloss:0.54449
    [6]	validation_0-logloss:0.52545
    [7]	validation_0-logloss:0.51119
    [8]	validation_0-logloss:0.50158
    [9]	validation_0-logloss:0.48818
    [10]	validation_0-logloss:0.48156
    [11]	validation_0-logloss:0.47088
    [12]	validation_0-logloss:0.46070
    [13]	validation_0-logloss:0.45777
    [14]	validation_0-logloss:0.44859
    [15]	validation_0-logloss:0.44451
    [16]	validation_0-logloss:0.43710
    [17]	validation_0-logloss:0.43314
    [18]	validation_0-logloss:0.42950
    [19]	validation_0-logloss:0.42641
    [20]	validation_0-logloss:0.42238
    [21]	validation_0-logloss:0.42042
    [22]	validation_0-logloss:0.41610
    [23]	validation_0-logloss:0.41212
    [24]	validation_0-logloss:0.40993
    [25]	validation_0-logloss:0.40805
    [26]	validation_0-logloss:0.40669
    [27]	validation_0-logloss:0.40407
    [28]	validation_0-logloss:0.40288
    [29]	validation_0-logloss:0.40087
    [30]	validation_0-logloss:0.39885
    [31]	validation_0-logloss:0.39851
    [32]	validation_0-logloss:0.39883
    [33]	validation_0-logloss:0.39677
    [34]	validation_0-logloss:0.39539
    [35]	validation_0-logloss:0.39531
    [36]	validation_0-logloss:0.39353
    [37]	validation_0-logloss:0.39271
    [38]	validation_0-logloss:0.39249
    [39]	validation_0-logloss:0.39254
    [40]	validation_0-logloss:0.39270
    [41]	validation_0-logloss:0.39218
    [42]	validation_0-logloss:0.39423
    [43]	validation_0-logloss:0.39426
    [44]	validation_0-logloss:0.39336
    [45]	validation_0-logloss:0.39049
    [46]	validation_0-logloss:0.39047
    [47]	validation_0-logloss:0.39099
    [48]	validation_0-logloss:0.39019
    [49]	validation_0-logloss:0.39002
    [50]	validation_0-logloss:0.39030
    [51]	validation_0-logloss:0.38909
    [52]	validation_0-logloss:0.38966
    [53]	validation_0-logloss:0.38967
    [54]	validation_0-logloss:0.38965
    [55]	validation_0-logloss:0.38951
    [56]	validation_0-logloss:0.39109
    [57]	validation_0-logloss:0.39089
    [58]	validation_0-logloss:0.39078
    [59]	validation_0-logloss:0.39051
    [60]	validation_0-logloss:0.38995
    [61]	validation_0-logloss:0.39049
    [62]	validation_0-logloss:0.39313
    [63]	validation_0-logloss:0.39524
    [64]	validation_0-logloss:0.39703
    [65]	validation_0-logloss:0.39723
    [66]	validation_0-logloss:0.39688
    [67]	validation_0-logloss:0.39664
    [68]	validation_0-logloss:0.39712
    [69]	validation_0-logloss:0.39705
    [70]	validation_0-logloss:0.39757
    [71]	validation_0-logloss:0.39758
    [72]	validation_0-logloss:0.39791
    [73]	validation_0-logloss:0.39708
    [74]	validation_0-logloss:0.39692
    [75]	validation_0-logloss:0.39734
    [76]	validation_0-logloss:0.39785
    [77]	validation_0-logloss:0.39799
    [78]	validation_0-logloss:0.39916
    [79]	validation_0-logloss:0.39920
    [80]	validation_0-logloss:0.39913
    [81]	validation_0-logloss:0.39909
    [82]	validation_0-logloss:0.39838
    [83]	validation_0-logloss:0.39855
    [84]	validation_0-logloss:0.39863
    [85]	validation_0-logloss:0.39842
    [86]	validation_0-logloss:0.39844
    [87]	validation_0-logloss:0.39898
    [88]	validation_0-logloss:0.39886
    [89]	validation_0-logloss:0.39974
    [90]	validation_0-logloss:0.39936
    [91]	validation_0-logloss:0.39929
    [92]	validation_0-logloss:0.39948
    [93]	validation_0-logloss:0.39969
    [94]	validation_0-logloss:0.40002
    [95]	validation_0-logloss:0.40040
    [96]	validation_0-logloss:0.40035
    [97]	validation_0-logloss:0.39962
    [98]	validation_0-logloss:0.39920
    [99]	validation_0-logloss:0.39820
    [02:01:56] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65945
    [1]	validation_0-logloss:0.63869
    [2]	validation_0-logloss:0.60535
    [3]	validation_0-logloss:0.58598
    [4]	validation_0-logloss:0.56909
    [5]	validation_0-logloss:0.55802
    [6]	validation_0-logloss:0.54097
    [7]	validation_0-logloss:0.52573
    [8]	validation_0-logloss:0.51597
    [9]	validation_0-logloss:0.50247
    [10]	validation_0-logloss:0.49558
    [11]	validation_0-logloss:0.48719
    [12]	validation_0-logloss:0.47832
    [13]	validation_0-logloss:0.47535
    [14]	validation_0-logloss:0.46660
    [15]	validation_0-logloss:0.46364
    [16]	validation_0-logloss:0.45827
    [17]	validation_0-logloss:0.45378
    [18]	validation_0-logloss:0.45131
    [19]	validation_0-logloss:0.44672
    [20]	validation_0-logloss:0.44207
    [21]	validation_0-logloss:0.44134
    [22]	validation_0-logloss:0.43828
    [23]	validation_0-logloss:0.43629
    [24]	validation_0-logloss:0.43306
    [25]	validation_0-logloss:0.43073
    [26]	validation_0-logloss:0.42991
    [27]	validation_0-logloss:0.42952
    [28]	validation_0-logloss:0.42869
    [29]	validation_0-logloss:0.42822
    [30]	validation_0-logloss:0.42641
    [31]	validation_0-logloss:0.42659
    [32]	validation_0-logloss:0.42725
    [33]	validation_0-logloss:0.42766
    [34]	validation_0-logloss:0.42774
    [35]	validation_0-logloss:0.42712
    [36]	validation_0-logloss:0.42696
    [37]	validation_0-logloss:0.42592
    [38]	validation_0-logloss:0.42600
    [39]	validation_0-logloss:0.42685
    [40]	validation_0-logloss:0.42845
    [41]	validation_0-logloss:0.42926
    [42]	validation_0-logloss:0.43099
    [43]	validation_0-logloss:0.43156
    [44]	validation_0-logloss:0.43025
    [45]	validation_0-logloss:0.43026
    [46]	validation_0-logloss:0.42972
    [47]	validation_0-logloss:0.42948
    [48]	validation_0-logloss:0.42815
    [49]	validation_0-logloss:0.42826
    [50]	validation_0-logloss:0.42792
    [51]	validation_0-logloss:0.43053
    [52]	validation_0-logloss:0.43084
    [53]	validation_0-logloss:0.43152
    [54]	validation_0-logloss:0.43276
    [55]	validation_0-logloss:0.43411
    [56]	validation_0-logloss:0.43381
    [57]	validation_0-logloss:0.43293
    [58]	validation_0-logloss:0.43245
    [59]	validation_0-logloss:0.43167
    [60]	validation_0-logloss:0.43047
    [61]	validation_0-logloss:0.43113
    [62]	validation_0-logloss:0.43179
    [63]	validation_0-logloss:0.43353
    [64]	validation_0-logloss:0.43278
    [65]	validation_0-logloss:0.43063
    [66]	validation_0-logloss:0.43013
    [67]	validation_0-logloss:0.43025
    [68]	validation_0-logloss:0.43056
    [69]	validation_0-logloss:0.42973
    [70]	validation_0-logloss:0.42694
    [71]	validation_0-logloss:0.42656
    [72]	validation_0-logloss:0.42694
    [73]	validation_0-logloss:0.42605
    [74]	validation_0-logloss:0.42769
    [75]	validation_0-logloss:0.42802
    [76]	validation_0-logloss:0.42782
    [77]	validation_0-logloss:0.42757
    [78]	validation_0-logloss:0.42854
    [79]	validation_0-logloss:0.42893
    [80]	validation_0-logloss:0.42912
    [81]	validation_0-logloss:0.42924
    [82]	validation_0-logloss:0.42951
    [83]	validation_0-logloss:0.42982
    [84]	validation_0-logloss:0.42995
    [85]	validation_0-logloss:0.43042
    [86]	validation_0-logloss:0.43042
    [87]	validation_0-logloss:0.43001
    [88]	validation_0-logloss:0.42925
    [89]	validation_0-logloss:0.43174
    [90]	validation_0-logloss:0.43106
    [91]	validation_0-logloss:0.43158
    [92]	validation_0-logloss:0.43113
    [93]	validation_0-logloss:0.43164
    [94]	validation_0-logloss:0.43129
    [95]	validation_0-logloss:0.43224
    [96]	validation_0-logloss:0.43269
    [97]	validation_0-logloss:0.43264
    [98]	validation_0-logloss:0.43275
    [99]	validation_0-logloss:0.43211
    [02:01:57] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65574
    [1]	validation_0-logloss:0.63197
    [2]	validation_0-logloss:0.59617
    [3]	validation_0-logloss:0.57619
    [4]	validation_0-logloss:0.55547
    [5]	validation_0-logloss:0.54289
    [6]	validation_0-logloss:0.52512
    [7]	validation_0-logloss:0.51064
    [8]	validation_0-logloss:0.50027
    [9]	validation_0-logloss:0.48637
    [10]	validation_0-logloss:0.48194
    [11]	validation_0-logloss:0.47265
    [12]	validation_0-logloss:0.46230
    [13]	validation_0-logloss:0.45681
    [14]	validation_0-logloss:0.44822
    [15]	validation_0-logloss:0.44540
    [16]	validation_0-logloss:0.44004
    [17]	validation_0-logloss:0.43600
    [18]	validation_0-logloss:0.43285
    [19]	validation_0-logloss:0.42919
    [20]	validation_0-logloss:0.42466
    [21]	validation_0-logloss:0.42436
    [22]	validation_0-logloss:0.42056
    [23]	validation_0-logloss:0.41735
    [24]	validation_0-logloss:0.41409
    [25]	validation_0-logloss:0.41307
    [26]	validation_0-logloss:0.41221
    [27]	validation_0-logloss:0.41191
    [28]	validation_0-logloss:0.41123
    [29]	validation_0-logloss:0.40970
    [30]	validation_0-logloss:0.40770
    [31]	validation_0-logloss:0.40740
    [32]	validation_0-logloss:0.40813
    [33]	validation_0-logloss:0.40714
    [34]	validation_0-logloss:0.40629
    [35]	validation_0-logloss:0.40549
    [36]	validation_0-logloss:0.40395
    [37]	validation_0-logloss:0.40336
    [38]	validation_0-logloss:0.40225
    [39]	validation_0-logloss:0.40321
    [40]	validation_0-logloss:0.40295
    [41]	validation_0-logloss:0.40263
    [42]	validation_0-logloss:0.40377
    [43]	validation_0-logloss:0.40373
    [44]	validation_0-logloss:0.40376
    [45]	validation_0-logloss:0.40345
    [46]	validation_0-logloss:0.40285
    [47]	validation_0-logloss:0.40329
    [48]	validation_0-logloss:0.40405
    [49]	validation_0-logloss:0.40432
    [50]	validation_0-logloss:0.40430
    [51]	validation_0-logloss:0.40450
    [52]	validation_0-logloss:0.40487
    [53]	validation_0-logloss:0.40455
    [54]	validation_0-logloss:0.40451
    [55]	validation_0-logloss:0.40541
    [56]	validation_0-logloss:0.40484
    [57]	validation_0-logloss:0.40386
    [58]	validation_0-logloss:0.40446
    [59]	validation_0-logloss:0.40446
    [60]	validation_0-logloss:0.40350
    [61]	validation_0-logloss:0.40309
    [62]	validation_0-logloss:0.40420
    [63]	validation_0-logloss:0.40499
    [64]	validation_0-logloss:0.40510
    [65]	validation_0-logloss:0.40631
    [66]	validation_0-logloss:0.40595
    [67]	validation_0-logloss:0.40659
    [68]	validation_0-logloss:0.40633
    [69]	validation_0-logloss:0.40644
    [70]	validation_0-logloss:0.40637
    [71]	validation_0-logloss:0.40615
    [72]	validation_0-logloss:0.40630
    [73]	validation_0-logloss:0.40503
    [74]	validation_0-logloss:0.40582
    [75]	validation_0-logloss:0.40594
    [76]	validation_0-logloss:0.40592
    [77]	validation_0-logloss:0.40650
    [78]	validation_0-logloss:0.40775
    [79]	validation_0-logloss:0.40778
    [80]	validation_0-logloss:0.40688
    [81]	validation_0-logloss:0.40677
    [82]	validation_0-logloss:0.40674
    [83]	validation_0-logloss:0.40666
    [84]	validation_0-logloss:0.40731
    [85]	validation_0-logloss:0.40811
    [86]	validation_0-logloss:0.40797
    [87]	validation_0-logloss:0.40802
    [88]	validation_0-logloss:0.40682
    [89]	validation_0-logloss:0.40738
    [90]	validation_0-logloss:0.40718
    [91]	validation_0-logloss:0.40712
    [92]	validation_0-logloss:0.40776
    [93]	validation_0-logloss:0.40773
    [94]	validation_0-logloss:0.40863
    [95]	validation_0-logloss:0.40889
    [96]	validation_0-logloss:0.40851
    [97]	validation_0-logloss:0.40833
    [98]	validation_0-logloss:0.40871
    [99]	validation_0-logloss:0.40749
    [02:01:57] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65704
    [1]	validation_0-logloss:0.63724
    [2]	validation_0-logloss:0.60104
    [3]	validation_0-logloss:0.58008
    [4]	validation_0-logloss:0.55864
    [5]	validation_0-logloss:0.54630
    [6]	validation_0-logloss:0.52863
    [7]	validation_0-logloss:0.51447
    [8]	validation_0-logloss:0.50426
    [9]	validation_0-logloss:0.49110
    [10]	validation_0-logloss:0.48407
    [11]	validation_0-logloss:0.47376
    [12]	validation_0-logloss:0.46204
    [13]	validation_0-logloss:0.45807
    [14]	validation_0-logloss:0.44950
    [15]	validation_0-logloss:0.44507
    [16]	validation_0-logloss:0.43855
    [17]	validation_0-logloss:0.43364
    [18]	validation_0-logloss:0.43046
    [19]	validation_0-logloss:0.42595
    [20]	validation_0-logloss:0.42193
    [21]	validation_0-logloss:0.42091
    [22]	validation_0-logloss:0.41719
    [23]	validation_0-logloss:0.41364
    [24]	validation_0-logloss:0.41123
    [25]	validation_0-logloss:0.41049
    [26]	validation_0-logloss:0.40982
    [27]	validation_0-logloss:0.40790
    [28]	validation_0-logloss:0.40721
    [29]	validation_0-logloss:0.40554
    [30]	validation_0-logloss:0.40321
    [31]	validation_0-logloss:0.40240
    [32]	validation_0-logloss:0.40313
    [33]	validation_0-logloss:0.40153
    [34]	validation_0-logloss:0.39934
    [35]	validation_0-logloss:0.39854
    [36]	validation_0-logloss:0.39746
    [37]	validation_0-logloss:0.39616
    [38]	validation_0-logloss:0.39582
    [39]	validation_0-logloss:0.39699
    [40]	validation_0-logloss:0.39679
    [41]	validation_0-logloss:0.39638
    [42]	validation_0-logloss:0.39642
    [43]	validation_0-logloss:0.39636
    [44]	validation_0-logloss:0.39511
    [45]	validation_0-logloss:0.39498
    [46]	validation_0-logloss:0.39515
    [47]	validation_0-logloss:0.39620
    [48]	validation_0-logloss:0.39570
    [49]	validation_0-logloss:0.39583
    [50]	validation_0-logloss:0.39637
    [51]	validation_0-logloss:0.39656
    [52]	validation_0-logloss:0.39648
    [53]	validation_0-logloss:0.39733
    [54]	validation_0-logloss:0.39457
    [55]	validation_0-logloss:0.39526
    [56]	validation_0-logloss:0.39499
    [57]	validation_0-logloss:0.39496
    [58]	validation_0-logloss:0.39536
    [59]	validation_0-logloss:0.39513
    [60]	validation_0-logloss:0.39493
    [61]	validation_0-logloss:0.39682
    [62]	validation_0-logloss:0.39725
    [63]	validation_0-logloss:0.39759
    [64]	validation_0-logloss:0.39813
    [65]	validation_0-logloss:0.39779
    [66]	validation_0-logloss:0.39790
    [67]	validation_0-logloss:0.39773
    [68]	validation_0-logloss:0.39766
    [69]	validation_0-logloss:0.39728
    [70]	validation_0-logloss:0.39723
    [71]	validation_0-logloss:0.39713
    [72]	validation_0-logloss:0.39723
    [73]	validation_0-logloss:0.39643
    [74]	validation_0-logloss:0.39440
    [75]	validation_0-logloss:0.39452
    [76]	validation_0-logloss:0.39444
    [77]	validation_0-logloss:0.39485
    [78]	validation_0-logloss:0.39415
    [79]	validation_0-logloss:0.39565
    [80]	validation_0-logloss:0.39500
    [81]	validation_0-logloss:0.39517
    [82]	validation_0-logloss:0.39441
    [83]	validation_0-logloss:0.39445
    [84]	validation_0-logloss:0.39593
    [85]	validation_0-logloss:0.39587
    [86]	validation_0-logloss:0.39645
    [87]	validation_0-logloss:0.39644
    [88]	validation_0-logloss:0.39551
    [89]	validation_0-logloss:0.39500
    [90]	validation_0-logloss:0.39447
    [91]	validation_0-logloss:0.39452
    [92]	validation_0-logloss:0.39453
    [93]	validation_0-logloss:0.39459
    [94]	validation_0-logloss:0.39409
    [95]	validation_0-logloss:0.39555
    [96]	validation_0-logloss:0.39623
    [97]	validation_0-logloss:0.39565
    [98]	validation_0-logloss:0.39575
    [99]	validation_0-logloss:0.39461
    [02:01:58] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.66156
    [1]	validation_0-logloss:0.64095
    [2]	validation_0-logloss:0.60697
    [3]	validation_0-logloss:0.58818
    [4]	validation_0-logloss:0.57028
    [5]	validation_0-logloss:0.55988
    [6]	validation_0-logloss:0.54634
    [7]	validation_0-logloss:0.53324
    [8]	validation_0-logloss:0.52316
    [9]	validation_0-logloss:0.50936
    [10]	validation_0-logloss:0.50340
    [11]	validation_0-logloss:0.49701
    [12]	validation_0-logloss:0.48865
    [13]	validation_0-logloss:0.48601
    [14]	validation_0-logloss:0.47741
    [15]	validation_0-logloss:0.47470
    [16]	validation_0-logloss:0.47050
    [17]	validation_0-logloss:0.46597
    [18]	validation_0-logloss:0.46400
    [19]	validation_0-logloss:0.46128
    [20]	validation_0-logloss:0.45643
    [21]	validation_0-logloss:0.45622
    [22]	validation_0-logloss:0.45243
    [23]	validation_0-logloss:0.44937
    [24]	validation_0-logloss:0.44621
    [25]	validation_0-logloss:0.44379
    [26]	validation_0-logloss:0.44382
    [27]	validation_0-logloss:0.44270
    [28]	validation_0-logloss:0.44203
    [29]	validation_0-logloss:0.44016
    [30]	validation_0-logloss:0.43825
    [31]	validation_0-logloss:0.43893
    [32]	validation_0-logloss:0.43965
    [33]	validation_0-logloss:0.43903
    [34]	validation_0-logloss:0.43922
    [35]	validation_0-logloss:0.43839
    [36]	validation_0-logloss:0.43824
    [37]	validation_0-logloss:0.43712
    [38]	validation_0-logloss:0.43707
    [39]	validation_0-logloss:0.43795
    [40]	validation_0-logloss:0.43888
    [41]	validation_0-logloss:0.43832
    [42]	validation_0-logloss:0.43966
    [43]	validation_0-logloss:0.43996
    [44]	validation_0-logloss:0.43903
    [45]	validation_0-logloss:0.43860
    [46]	validation_0-logloss:0.43826
    [47]	validation_0-logloss:0.43969
    [48]	validation_0-logloss:0.43885
    [49]	validation_0-logloss:0.43723
    [50]	validation_0-logloss:0.43740
    [51]	validation_0-logloss:0.43675
    [52]	validation_0-logloss:0.43594
    [53]	validation_0-logloss:0.43665
    [54]	validation_0-logloss:0.43659
    [55]	validation_0-logloss:0.43779
    [56]	validation_0-logloss:0.43635
    [57]	validation_0-logloss:0.43566
    [58]	validation_0-logloss:0.43429
    [59]	validation_0-logloss:0.43332
    [60]	validation_0-logloss:0.43246
    [61]	validation_0-logloss:0.43410
    [62]	validation_0-logloss:0.43395
    [63]	validation_0-logloss:0.43500
    [64]	validation_0-logloss:0.43501
    [65]	validation_0-logloss:0.43415
    [66]	validation_0-logloss:0.43472
    [67]	validation_0-logloss:0.43630
    [68]	validation_0-logloss:0.43641
    [69]	validation_0-logloss:0.43683
    [70]	validation_0-logloss:0.43623
    [71]	validation_0-logloss:0.43532
    [72]	validation_0-logloss:0.43562
    [73]	validation_0-logloss:0.43463
    [74]	validation_0-logloss:0.43407
    [75]	validation_0-logloss:0.43493
    [76]	validation_0-logloss:0.43480
    [77]	validation_0-logloss:0.43494
    [78]	validation_0-logloss:0.43506
    [79]	validation_0-logloss:0.43568
    [80]	validation_0-logloss:0.43561
    [81]	validation_0-logloss:0.43564
    [82]	validation_0-logloss:0.43505
    [83]	validation_0-logloss:0.43528
    [84]	validation_0-logloss:0.43757
    [85]	validation_0-logloss:0.43799
    [86]	validation_0-logloss:0.43672
    [87]	validation_0-logloss:0.43694
    [88]	validation_0-logloss:0.43615
    [89]	validation_0-logloss:0.43753
    [90]	validation_0-logloss:0.43662
    [91]	validation_0-logloss:0.43680
    [92]	validation_0-logloss:0.43659
    [93]	validation_0-logloss:0.43677
    [94]	validation_0-logloss:0.43653
    [95]	validation_0-logloss:0.43584
    [96]	validation_0-logloss:0.43575
    [97]	validation_0-logloss:0.43573
    [98]	validation_0-logloss:0.43556
    [99]	validation_0-logloss:0.43479
    [02:01:59] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65375
    [1]	validation_0-logloss:0.62965
    [2]	validation_0-logloss:0.59412
    [3]	validation_0-logloss:0.57352
    [4]	validation_0-logloss:0.55322
    [5]	validation_0-logloss:0.54023
    [6]	validation_0-logloss:0.52144
    [7]	validation_0-logloss:0.50563
    [8]	validation_0-logloss:0.49530
    [9]	validation_0-logloss:0.48175
    [10]	validation_0-logloss:0.47482
    [11]	validation_0-logloss:0.46434
    [12]	validation_0-logloss:0.45378
    [13]	validation_0-logloss:0.44790
    [14]	validation_0-logloss:0.43952
    [15]	validation_0-logloss:0.43697
    [16]	validation_0-logloss:0.43162
    [17]	validation_0-logloss:0.42745
    [18]	validation_0-logloss:0.42433
    [19]	validation_0-logloss:0.41900
    [20]	validation_0-logloss:0.41477
    [21]	validation_0-logloss:0.41416
    [22]	validation_0-logloss:0.41028
    [23]	validation_0-logloss:0.40710
    [24]	validation_0-logloss:0.40465
    [25]	validation_0-logloss:0.40333
    [26]	validation_0-logloss:0.40203
    [27]	validation_0-logloss:0.40206
    [28]	validation_0-logloss:0.40117
    [29]	validation_0-logloss:0.39950
    [30]	validation_0-logloss:0.39775
    [31]	validation_0-logloss:0.39830
    [32]	validation_0-logloss:0.39932
    [33]	validation_0-logloss:0.39951
    [34]	validation_0-logloss:0.40090
    [35]	validation_0-logloss:0.40096
    [36]	validation_0-logloss:0.40116
    [37]	validation_0-logloss:0.40029
    [38]	validation_0-logloss:0.40081
    [39]	validation_0-logloss:0.40283
    [40]	validation_0-logloss:0.40383
    [41]	validation_0-logloss:0.40341
    [42]	validation_0-logloss:0.40481
    [43]	validation_0-logloss:0.40604
    [44]	validation_0-logloss:0.40524
    [45]	validation_0-logloss:0.40558
    [46]	validation_0-logloss:0.40546
    [47]	validation_0-logloss:0.40835
    [48]	validation_0-logloss:0.40858
    [49]	validation_0-logloss:0.40799
    [50]	validation_0-logloss:0.40953
    [51]	validation_0-logloss:0.40964
    [52]	validation_0-logloss:0.40993
    [53]	validation_0-logloss:0.41077
    [54]	validation_0-logloss:0.41092
    [55]	validation_0-logloss:0.41218
    [56]	validation_0-logloss:0.41322
    [57]	validation_0-logloss:0.41244
    [58]	validation_0-logloss:0.41246
    [59]	validation_0-logloss:0.41175
    [60]	validation_0-logloss:0.41112
    [61]	validation_0-logloss:0.41158
    [62]	validation_0-logloss:0.41315
    [63]	validation_0-logloss:0.41442
    [64]	validation_0-logloss:0.41452
    [65]	validation_0-logloss:0.41451
    [66]	validation_0-logloss:0.41596
    [67]	validation_0-logloss:0.41696
    [68]	validation_0-logloss:0.41689
    [69]	validation_0-logloss:0.41746
    [70]	validation_0-logloss:0.41855
    [71]	validation_0-logloss:0.41938
    [72]	validation_0-logloss:0.41975
    [73]	validation_0-logloss:0.41935
    [74]	validation_0-logloss:0.41940
    [75]	validation_0-logloss:0.41965
    [76]	validation_0-logloss:0.42057
    [77]	validation_0-logloss:0.42147
    [78]	validation_0-logloss:0.42187
    [79]	validation_0-logloss:0.42424
    [80]	validation_0-logloss:0.42617
    [81]	validation_0-logloss:0.42656
    [82]	validation_0-logloss:0.42592
    [83]	validation_0-logloss:0.42606
    [84]	validation_0-logloss:0.42741
    [85]	validation_0-logloss:0.42825
    [86]	validation_0-logloss:0.42836
    [87]	validation_0-logloss:0.42794
    [88]	validation_0-logloss:0.42725
    [89]	validation_0-logloss:0.42772
    [90]	validation_0-logloss:0.42714
    [91]	validation_0-logloss:0.42726
    [92]	validation_0-logloss:0.42885
    [93]	validation_0-logloss:0.42901
    [94]	validation_0-logloss:0.42962
    [95]	validation_0-logloss:0.43031
    [96]	validation_0-logloss:0.43197
    [97]	validation_0-logloss:0.43160
    [98]	validation_0-logloss:0.43225
    [99]	validation_0-logloss:0.43162
    [02:01:59] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65453
    [1]	validation_0-logloss:0.63219
    [2]	validation_0-logloss:0.59642
    [3]	validation_0-logloss:0.57559
    [4]	validation_0-logloss:0.55521
    [5]	validation_0-logloss:0.54294
    [6]	validation_0-logloss:0.52416
    [7]	validation_0-logloss:0.50880
    [8]	validation_0-logloss:0.49919
    [9]	validation_0-logloss:0.48595
    [10]	validation_0-logloss:0.47847
    [11]	validation_0-logloss:0.46697
    [12]	validation_0-logloss:0.45589
    [13]	validation_0-logloss:0.45113
    [14]	validation_0-logloss:0.44209
    [15]	validation_0-logloss:0.43873
    [16]	validation_0-logloss:0.43136
    [17]	validation_0-logloss:0.42715
    [18]	validation_0-logloss:0.42407
    [19]	validation_0-logloss:0.41990
    [20]	validation_0-logloss:0.41597
    [21]	validation_0-logloss:0.41444
    [22]	validation_0-logloss:0.40942
    [23]	validation_0-logloss:0.40566
    [24]	validation_0-logloss:0.40343
    [25]	validation_0-logloss:0.40230
    [26]	validation_0-logloss:0.40025
    [27]	validation_0-logloss:0.39939
    [28]	validation_0-logloss:0.39811
    [29]	validation_0-logloss:0.39578
    [30]	validation_0-logloss:0.39380
    [31]	validation_0-logloss:0.39421
    [32]	validation_0-logloss:0.39519
    [33]	validation_0-logloss:0.39434
    [34]	validation_0-logloss:0.39136
    [35]	validation_0-logloss:0.39140
    [36]	validation_0-logloss:0.38933
    [37]	validation_0-logloss:0.38792
    [38]	validation_0-logloss:0.38713
    [39]	validation_0-logloss:0.38681
    [40]	validation_0-logloss:0.38770
    [41]	validation_0-logloss:0.38721
    [42]	validation_0-logloss:0.38944
    [43]	validation_0-logloss:0.38874
    [44]	validation_0-logloss:0.38880
    [45]	validation_0-logloss:0.38790
    [46]	validation_0-logloss:0.38816
    [47]	validation_0-logloss:0.38853
    [48]	validation_0-logloss:0.38959
    [49]	validation_0-logloss:0.38935
    [50]	validation_0-logloss:0.38907
    [51]	validation_0-logloss:0.38713
    [52]	validation_0-logloss:0.38708
    [53]	validation_0-logloss:0.38942
    [54]	validation_0-logloss:0.38931
    [55]	validation_0-logloss:0.39054
    [56]	validation_0-logloss:0.39039
    [57]	validation_0-logloss:0.38949
    [58]	validation_0-logloss:0.38997
    [59]	validation_0-logloss:0.38958
    [60]	validation_0-logloss:0.38923
    [61]	validation_0-logloss:0.38977
    [62]	validation_0-logloss:0.39119
    [63]	validation_0-logloss:0.39206
    [64]	validation_0-logloss:0.39191
    [65]	validation_0-logloss:0.39311
    [66]	validation_0-logloss:0.39377
    [67]	validation_0-logloss:0.39534
    [68]	validation_0-logloss:0.39550
    [69]	validation_0-logloss:0.39523
    [70]	validation_0-logloss:0.39583
    [71]	validation_0-logloss:0.39568
    [72]	validation_0-logloss:0.39481
    [73]	validation_0-logloss:0.39419
    [74]	validation_0-logloss:0.39575
    [75]	validation_0-logloss:0.39607
    [76]	validation_0-logloss:0.39611
    [77]	validation_0-logloss:0.39620
    [78]	validation_0-logloss:0.39647
    [79]	validation_0-logloss:0.39785
    [80]	validation_0-logloss:0.39859
    [81]	validation_0-logloss:0.39900
    [82]	validation_0-logloss:0.39864
    [83]	validation_0-logloss:0.39848
    [84]	validation_0-logloss:0.39971
    [85]	validation_0-logloss:0.39988
    [86]	validation_0-logloss:0.40012
    [87]	validation_0-logloss:0.40058
    [88]	validation_0-logloss:0.40001
    [89]	validation_0-logloss:0.40141
    [90]	validation_0-logloss:0.40104
    [91]	validation_0-logloss:0.40093
    [92]	validation_0-logloss:0.40103
    [93]	validation_0-logloss:0.40095
    [94]	validation_0-logloss:0.40133
    [95]	validation_0-logloss:0.40138
    [96]	validation_0-logloss:0.40062
    [97]	validation_0-logloss:0.39995
    [98]	validation_0-logloss:0.39996
    [99]	validation_0-logloss:0.39902
    [02:02:00] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65945
    [1]	validation_0-logloss:0.63936
    [2]	validation_0-logloss:0.60605
    [3]	validation_0-logloss:0.58668
    [4]	validation_0-logloss:0.56957
    [5]	validation_0-logloss:0.55889
    [6]	validation_0-logloss:0.54215
    [7]	validation_0-logloss:0.52469
    [8]	validation_0-logloss:0.51499
    [9]	validation_0-logloss:0.50174
    [10]	validation_0-logloss:0.49430
    [11]	validation_0-logloss:0.48714
    [12]	validation_0-logloss:0.47780
    [13]	validation_0-logloss:0.47525
    [14]	validation_0-logloss:0.46650
    [15]	validation_0-logloss:0.46481
    [16]	validation_0-logloss:0.45773
    [17]	validation_0-logloss:0.45336
    [18]	validation_0-logloss:0.45206
    [19]	validation_0-logloss:0.44759
    [20]	validation_0-logloss:0.44315
    [21]	validation_0-logloss:0.44230
    [22]	validation_0-logloss:0.43810
    [23]	validation_0-logloss:0.43638
    [24]	validation_0-logloss:0.43321
    [25]	validation_0-logloss:0.43105
    [26]	validation_0-logloss:0.43072
    [27]	validation_0-logloss:0.42887
    [28]	validation_0-logloss:0.42808
    [29]	validation_0-logloss:0.42725
    [30]	validation_0-logloss:0.42563
    [31]	validation_0-logloss:0.42721
    [32]	validation_0-logloss:0.42796
    [33]	validation_0-logloss:0.42811
    [34]	validation_0-logloss:0.42739
    [35]	validation_0-logloss:0.42841
    [36]	validation_0-logloss:0.42705
    [37]	validation_0-logloss:0.42589
    [38]	validation_0-logloss:0.42560
    [39]	validation_0-logloss:0.42566
    [40]	validation_0-logloss:0.42656
    [41]	validation_0-logloss:0.42709
    [42]	validation_0-logloss:0.42933
    [43]	validation_0-logloss:0.42902
    [44]	validation_0-logloss:0.42830
    [45]	validation_0-logloss:0.42783
    [46]	validation_0-logloss:0.42810
    [47]	validation_0-logloss:0.42888
    [48]	validation_0-logloss:0.42782
    [49]	validation_0-logloss:0.42768
    [50]	validation_0-logloss:0.42804
    [51]	validation_0-logloss:0.42965
    [52]	validation_0-logloss:0.42936
    [53]	validation_0-logloss:0.42966
    [54]	validation_0-logloss:0.43039
    [55]	validation_0-logloss:0.43219
    [56]	validation_0-logloss:0.43128
    [57]	validation_0-logloss:0.43004
    [58]	validation_0-logloss:0.42888
    [59]	validation_0-logloss:0.42821
    [60]	validation_0-logloss:0.42711
    [61]	validation_0-logloss:0.42903
    [62]	validation_0-logloss:0.43062
    [63]	validation_0-logloss:0.43097
    [64]	validation_0-logloss:0.42994
    [65]	validation_0-logloss:0.42952
    [66]	validation_0-logloss:0.43013
    [67]	validation_0-logloss:0.43172
    [68]	validation_0-logloss:0.43196
    [69]	validation_0-logloss:0.43027
    [70]	validation_0-logloss:0.42769
    [71]	validation_0-logloss:0.42649
    [72]	validation_0-logloss:0.42604
    [73]	validation_0-logloss:0.42530
    [74]	validation_0-logloss:0.42601
    [75]	validation_0-logloss:0.42629
    [76]	validation_0-logloss:0.42436
    [77]	validation_0-logloss:0.42492
    [78]	validation_0-logloss:0.42570
    [79]	validation_0-logloss:0.42616
    [80]	validation_0-logloss:0.42615
    [81]	validation_0-logloss:0.42439
    [82]	validation_0-logloss:0.42391
    [83]	validation_0-logloss:0.42435
    [84]	validation_0-logloss:0.42585
    [85]	validation_0-logloss:0.42644
    [86]	validation_0-logloss:0.42656
    [87]	validation_0-logloss:0.42642
    [88]	validation_0-logloss:0.42580
    [89]	validation_0-logloss:0.42819
    [90]	validation_0-logloss:0.42781
    [91]	validation_0-logloss:0.42827
    [92]	validation_0-logloss:0.42837
    [93]	validation_0-logloss:0.42881
    [94]	validation_0-logloss:0.42881
    [95]	validation_0-logloss:0.43028
    [96]	validation_0-logloss:0.43078
    [97]	validation_0-logloss:0.43042
    [98]	validation_0-logloss:0.43004
    [99]	validation_0-logloss:0.42946
    [02:02:01] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65574
    [1]	validation_0-logloss:0.63136
    [2]	validation_0-logloss:0.59560
    [3]	validation_0-logloss:0.57517
    [4]	validation_0-logloss:0.55466
    [5]	validation_0-logloss:0.54152
    [6]	validation_0-logloss:0.52382
    [7]	validation_0-logloss:0.50917
    [8]	validation_0-logloss:0.49875
    [9]	validation_0-logloss:0.48497
    [10]	validation_0-logloss:0.48053
    [11]	validation_0-logloss:0.47141
    [12]	validation_0-logloss:0.46126
    [13]	validation_0-logloss:0.45562
    [14]	validation_0-logloss:0.44712
    [15]	validation_0-logloss:0.44415
    [16]	validation_0-logloss:0.43914
    [17]	validation_0-logloss:0.43506
    [18]	validation_0-logloss:0.43338
    [19]	validation_0-logloss:0.42951
    [20]	validation_0-logloss:0.42497
    [21]	validation_0-logloss:0.42418
    [22]	validation_0-logloss:0.41963
    [23]	validation_0-logloss:0.41678
    [24]	validation_0-logloss:0.41410
    [25]	validation_0-logloss:0.41259
    [26]	validation_0-logloss:0.41146
    [27]	validation_0-logloss:0.41017
    [28]	validation_0-logloss:0.40954
    [29]	validation_0-logloss:0.40810
    [30]	validation_0-logloss:0.40652
    [31]	validation_0-logloss:0.40535
    [32]	validation_0-logloss:0.40648
    [33]	validation_0-logloss:0.40579
    [34]	validation_0-logloss:0.40659
    [35]	validation_0-logloss:0.40652
    [36]	validation_0-logloss:0.40521
    [37]	validation_0-logloss:0.40643
    [38]	validation_0-logloss:0.40534
    [39]	validation_0-logloss:0.40682
    [40]	validation_0-logloss:0.40711
    [41]	validation_0-logloss:0.40692
    [42]	validation_0-logloss:0.40754
    [43]	validation_0-logloss:0.40745
    [44]	validation_0-logloss:0.40719
    [45]	validation_0-logloss:0.40746
    [46]	validation_0-logloss:0.40689
    [47]	validation_0-logloss:0.40824
    [48]	validation_0-logloss:0.40854
    [49]	validation_0-logloss:0.40909
    [50]	validation_0-logloss:0.40970
    [51]	validation_0-logloss:0.41041
    [52]	validation_0-logloss:0.41004
    [53]	validation_0-logloss:0.40960
    [54]	validation_0-logloss:0.40879
    [55]	validation_0-logloss:0.40872
    [56]	validation_0-logloss:0.40980
    [57]	validation_0-logloss:0.40854
    [58]	validation_0-logloss:0.40994
    [59]	validation_0-logloss:0.40879
    [60]	validation_0-logloss:0.40775
    [61]	validation_0-logloss:0.40795
    [62]	validation_0-logloss:0.40866
    [63]	validation_0-logloss:0.40912
    [64]	validation_0-logloss:0.40892
    [65]	validation_0-logloss:0.40940
    [66]	validation_0-logloss:0.40816
    [67]	validation_0-logloss:0.40877
    [68]	validation_0-logloss:0.40851
    [69]	validation_0-logloss:0.40837
    [70]	validation_0-logloss:0.40941
    [71]	validation_0-logloss:0.40989
    [72]	validation_0-logloss:0.41037
    [73]	validation_0-logloss:0.40915
    [74]	validation_0-logloss:0.40812
    [75]	validation_0-logloss:0.40856
    [76]	validation_0-logloss:0.40928
    [77]	validation_0-logloss:0.40956
    [78]	validation_0-logloss:0.41123
    [79]	validation_0-logloss:0.41125
    [80]	validation_0-logloss:0.41200
    [81]	validation_0-logloss:0.41223
    [82]	validation_0-logloss:0.41195
    [83]	validation_0-logloss:0.41208
    [84]	validation_0-logloss:0.41212
    [85]	validation_0-logloss:0.41249
    [86]	validation_0-logloss:0.41185
    [87]	validation_0-logloss:0.41227
    [88]	validation_0-logloss:0.41118
    [89]	validation_0-logloss:0.41182
    [90]	validation_0-logloss:0.41162
    [91]	validation_0-logloss:0.41174
    [92]	validation_0-logloss:0.41203
    [93]	validation_0-logloss:0.41198
    [94]	validation_0-logloss:0.41182
    [95]	validation_0-logloss:0.41173
    [96]	validation_0-logloss:0.41311
    [97]	validation_0-logloss:0.41294
    [98]	validation_0-logloss:0.41334
    [99]	validation_0-logloss:0.41217
    [02:02:01] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65704
    [1]	validation_0-logloss:0.63583
    [2]	validation_0-logloss:0.59986
    [3]	validation_0-logloss:0.57909
    [4]	validation_0-logloss:0.55777
    [5]	validation_0-logloss:0.54652
    [6]	validation_0-logloss:0.52868
    [7]	validation_0-logloss:0.51466
    [8]	validation_0-logloss:0.50423
    [9]	validation_0-logloss:0.49099
    [10]	validation_0-logloss:0.48456
    [11]	validation_0-logloss:0.47413
    [12]	validation_0-logloss:0.46276
    [13]	validation_0-logloss:0.45871
    [14]	validation_0-logloss:0.44995
    [15]	validation_0-logloss:0.44602
    [16]	validation_0-logloss:0.43964
    [17]	validation_0-logloss:0.43444
    [18]	validation_0-logloss:0.43217
    [19]	validation_0-logloss:0.42736
    [20]	validation_0-logloss:0.42300
    [21]	validation_0-logloss:0.42263
    [22]	validation_0-logloss:0.41806
    [23]	validation_0-logloss:0.41442
    [24]	validation_0-logloss:0.41198
    [25]	validation_0-logloss:0.41014
    [26]	validation_0-logloss:0.40950
    [27]	validation_0-logloss:0.40915
    [28]	validation_0-logloss:0.40828
    [29]	validation_0-logloss:0.40659
    [30]	validation_0-logloss:0.40434
    [31]	validation_0-logloss:0.40259
    [32]	validation_0-logloss:0.40288
    [33]	validation_0-logloss:0.40251
    [34]	validation_0-logloss:0.40221
    [35]	validation_0-logloss:0.40165
    [36]	validation_0-logloss:0.40058
    [37]	validation_0-logloss:0.39958
    [38]	validation_0-logloss:0.39865
    [39]	validation_0-logloss:0.39938
    [40]	validation_0-logloss:0.40000
    [41]	validation_0-logloss:0.39958
    [42]	validation_0-logloss:0.39943
    [43]	validation_0-logloss:0.39873
    [44]	validation_0-logloss:0.39794
    [45]	validation_0-logloss:0.39617
    [46]	validation_0-logloss:0.39543
    [47]	validation_0-logloss:0.39756
    [48]	validation_0-logloss:0.39710
    [49]	validation_0-logloss:0.39587
    [50]	validation_0-logloss:0.39549
    [51]	validation_0-logloss:0.39367
    [52]	validation_0-logloss:0.39359
    [53]	validation_0-logloss:0.39407
    [54]	validation_0-logloss:0.39212
    [55]	validation_0-logloss:0.39341
    [56]	validation_0-logloss:0.39321
    [57]	validation_0-logloss:0.39231
    [58]	validation_0-logloss:0.39262
    [59]	validation_0-logloss:0.39190
    [60]	validation_0-logloss:0.39109
    [61]	validation_0-logloss:0.39169
    [62]	validation_0-logloss:0.39361
    [63]	validation_0-logloss:0.39467
    [64]	validation_0-logloss:0.39462
    [65]	validation_0-logloss:0.39443
    [66]	validation_0-logloss:0.39510
    [67]	validation_0-logloss:0.39534
    [68]	validation_0-logloss:0.39517
    [69]	validation_0-logloss:0.39484
    [70]	validation_0-logloss:0.39503
    [71]	validation_0-logloss:0.39494
    [72]	validation_0-logloss:0.39603
    [73]	validation_0-logloss:0.39536
    [74]	validation_0-logloss:0.39651
    [75]	validation_0-logloss:0.39664
    [76]	validation_0-logloss:0.39683
    [77]	validation_0-logloss:0.39743
    [78]	validation_0-logloss:0.39915
    [79]	validation_0-logloss:0.40081
    [80]	validation_0-logloss:0.39948
    [81]	validation_0-logloss:0.40006
    [82]	validation_0-logloss:0.39929
    [83]	validation_0-logloss:0.39920
    [84]	validation_0-logloss:0.40091
    [85]	validation_0-logloss:0.40102
    [86]	validation_0-logloss:0.40065
    [87]	validation_0-logloss:0.40004
    [88]	validation_0-logloss:0.39942
    [89]	validation_0-logloss:0.40110
    [90]	validation_0-logloss:0.40038
    [91]	validation_0-logloss:0.40037
    [92]	validation_0-logloss:0.39995
    [93]	validation_0-logloss:0.39994
    [94]	validation_0-logloss:0.39956
    [95]	validation_0-logloss:0.39897
    [96]	validation_0-logloss:0.39796
    [97]	validation_0-logloss:0.39795
    [98]	validation_0-logloss:0.39840
    [99]	validation_0-logloss:0.39733
    [02:02:02] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.66156
    [1]	validation_0-logloss:0.64134
    [2]	validation_0-logloss:0.60735
    [3]	validation_0-logloss:0.58788
    [4]	validation_0-logloss:0.56993
    [5]	validation_0-logloss:0.55958
    [6]	validation_0-logloss:0.54440
    [7]	validation_0-logloss:0.52948
    [8]	validation_0-logloss:0.51939
    [9]	validation_0-logloss:0.50581
    [10]	validation_0-logloss:0.50046
    [11]	validation_0-logloss:0.49482
    [12]	validation_0-logloss:0.48484
    [13]	validation_0-logloss:0.48299
    [14]	validation_0-logloss:0.47450
    [15]	validation_0-logloss:0.47255
    [16]	validation_0-logloss:0.46767
    [17]	validation_0-logloss:0.46307
    [18]	validation_0-logloss:0.46119
    [19]	validation_0-logloss:0.45844
    [20]	validation_0-logloss:0.45375
    [21]	validation_0-logloss:0.45262
    [22]	validation_0-logloss:0.44968
    [23]	validation_0-logloss:0.44647
    [24]	validation_0-logloss:0.44333
    [25]	validation_0-logloss:0.44220
    [26]	validation_0-logloss:0.44129
    [27]	validation_0-logloss:0.43976
    [28]	validation_0-logloss:0.43907
    [29]	validation_0-logloss:0.43722
    [30]	validation_0-logloss:0.43528
    [31]	validation_0-logloss:0.43561
    [32]	validation_0-logloss:0.43566
    [33]	validation_0-logloss:0.43389
    [34]	validation_0-logloss:0.43473
    [35]	validation_0-logloss:0.43499
    [36]	validation_0-logloss:0.43510
    [37]	validation_0-logloss:0.43513
    [38]	validation_0-logloss:0.43538
    [39]	validation_0-logloss:0.43684
    [40]	validation_0-logloss:0.43672
    [41]	validation_0-logloss:0.43612
    [42]	validation_0-logloss:0.43767
    [43]	validation_0-logloss:0.43732
    [44]	validation_0-logloss:0.43702
    [45]	validation_0-logloss:0.43710
    [46]	validation_0-logloss:0.43608
    [47]	validation_0-logloss:0.43698
    [48]	validation_0-logloss:0.43648
    [49]	validation_0-logloss:0.43691
    [50]	validation_0-logloss:0.43699
    [51]	validation_0-logloss:0.43766
    [52]	validation_0-logloss:0.43856
    [53]	validation_0-logloss:0.43968
    [54]	validation_0-logloss:0.43969
    [55]	validation_0-logloss:0.44187
    [56]	validation_0-logloss:0.44238
    [57]	validation_0-logloss:0.44149
    [58]	validation_0-logloss:0.44149
    [59]	validation_0-logloss:0.44067
    [60]	validation_0-logloss:0.43993
    [61]	validation_0-logloss:0.44129
    [62]	validation_0-logloss:0.44292
    [63]	validation_0-logloss:0.44360
    [64]	validation_0-logloss:0.44288
    [65]	validation_0-logloss:0.44272
    [66]	validation_0-logloss:0.44368
    [67]	validation_0-logloss:0.44521
    [68]	validation_0-logloss:0.44524
    [69]	validation_0-logloss:0.44595
    [70]	validation_0-logloss:0.44608
    [71]	validation_0-logloss:0.44487
    [72]	validation_0-logloss:0.44433
    [73]	validation_0-logloss:0.44330
    [74]	validation_0-logloss:0.44311
    [75]	validation_0-logloss:0.44387
    [76]	validation_0-logloss:0.44330
    [77]	validation_0-logloss:0.44305
    [78]	validation_0-logloss:0.44322
    [79]	validation_0-logloss:0.44478
    [80]	validation_0-logloss:0.44613
    [81]	validation_0-logloss:0.44521
    [82]	validation_0-logloss:0.44526
    [83]	validation_0-logloss:0.44551
    [84]	validation_0-logloss:0.44673
    [85]	validation_0-logloss:0.44719
    [86]	validation_0-logloss:0.44670
    [87]	validation_0-logloss:0.44678
    [88]	validation_0-logloss:0.44584
    [89]	validation_0-logloss:0.44746
    [90]	validation_0-logloss:0.44636
    [91]	validation_0-logloss:0.44661
    [92]	validation_0-logloss:0.44621
    [93]	validation_0-logloss:0.44646
    [94]	validation_0-logloss:0.44667
    [95]	validation_0-logloss:0.44653
    [96]	validation_0-logloss:0.44649
    [97]	validation_0-logloss:0.44546
    [98]	validation_0-logloss:0.44515
    [99]	validation_0-logloss:0.44425
    [02:02:03] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64544
    [1]	validation_0-logloss:0.62229
    [2]	validation_0-logloss:0.58657
    [3]	validation_0-logloss:0.56247
    [4]	validation_0-logloss:0.53719
    [5]	validation_0-logloss:0.52522
    [6]	validation_0-logloss:0.50556
    [7]	validation_0-logloss:0.48924
    [8]	validation_0-logloss:0.47722
    [9]	validation_0-logloss:0.46468
    [10]	validation_0-logloss:0.45559
    [11]	validation_0-logloss:0.44681
    [12]	validation_0-logloss:0.43801
    [13]	validation_0-logloss:0.42994
    [14]	validation_0-logloss:0.42445
    [15]	validation_0-logloss:0.41946
    [16]	validation_0-logloss:0.41576
    [17]	validation_0-logloss:0.41156
    [18]	validation_0-logloss:0.40804
    [19]	validation_0-logloss:0.40680
    [20]	validation_0-logloss:0.40515
    [21]	validation_0-logloss:0.40409
    [22]	validation_0-logloss:0.40274
    [23]	validation_0-logloss:0.40083
    [24]	validation_0-logloss:0.39862
    [25]	validation_0-logloss:0.39712
    [26]	validation_0-logloss:0.39629
    [27]	validation_0-logloss:0.39600
    [28]	validation_0-logloss:0.39590
    [29]	validation_0-logloss:0.39532
    [30]	validation_0-logloss:0.39644
    [31]	validation_0-logloss:0.39747
    [32]	validation_0-logloss:0.39846
    [33]	validation_0-logloss:0.39919
    [34]	validation_0-logloss:0.39874
    [35]	validation_0-logloss:0.39822
    [36]	validation_0-logloss:0.40029
    [37]	validation_0-logloss:0.40203
    [38]	validation_0-logloss:0.40102
    [39]	validation_0-logloss:0.40230
    [40]	validation_0-logloss:0.40236
    [41]	validation_0-logloss:0.40221
    [42]	validation_0-logloss:0.40316
    [43]	validation_0-logloss:0.40238
    [44]	validation_0-logloss:0.40328
    [45]	validation_0-logloss:0.40532
    [46]	validation_0-logloss:0.40353
    [47]	validation_0-logloss:0.40454
    [48]	validation_0-logloss:0.40519
    [49]	validation_0-logloss:0.40512
    [50]	validation_0-logloss:0.40484
    [51]	validation_0-logloss:0.40651
    [52]	validation_0-logloss:0.40683
    [53]	validation_0-logloss:0.40847
    [54]	validation_0-logloss:0.40986
    [55]	validation_0-logloss:0.40988
    [56]	validation_0-logloss:0.41153
    [57]	validation_0-logloss:0.41159
    [58]	validation_0-logloss:0.41196
    [59]	validation_0-logloss:0.41195
    [60]	validation_0-logloss:0.41240
    [61]	validation_0-logloss:0.41345
    [62]	validation_0-logloss:0.41537
    [63]	validation_0-logloss:0.41591
    [64]	validation_0-logloss:0.41699
    [65]	validation_0-logloss:0.41837
    [66]	validation_0-logloss:0.41979
    [67]	validation_0-logloss:0.42070
    [68]	validation_0-logloss:0.42161
    [69]	validation_0-logloss:0.42140
    [70]	validation_0-logloss:0.42123
    [71]	validation_0-logloss:0.42066
    [72]	validation_0-logloss:0.42075
    [73]	validation_0-logloss:0.42149
    [74]	validation_0-logloss:0.42221
    [75]	validation_0-logloss:0.42401
    [76]	validation_0-logloss:0.42517
    [77]	validation_0-logloss:0.42532
    [78]	validation_0-logloss:0.42455
    [79]	validation_0-logloss:0.42505
    [80]	validation_0-logloss:0.42553
    [81]	validation_0-logloss:0.42528
    [82]	validation_0-logloss:0.42594
    [83]	validation_0-logloss:0.42706
    [84]	validation_0-logloss:0.42805
    [85]	validation_0-logloss:0.42862
    [86]	validation_0-logloss:0.43052
    [87]	validation_0-logloss:0.43146
    [88]	validation_0-logloss:0.43261
    [89]	validation_0-logloss:0.43293
    [90]	validation_0-logloss:0.43333
    [91]	validation_0-logloss:0.43303
    [92]	validation_0-logloss:0.43405
    [93]	validation_0-logloss:0.43408
    [94]	validation_0-logloss:0.43404
    [95]	validation_0-logloss:0.43512
    [96]	validation_0-logloss:0.43611
    [97]	validation_0-logloss:0.43642
    [98]	validation_0-logloss:0.43766
    [99]	validation_0-logloss:0.43929
    [02:02:03] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64795
    [1]	validation_0-logloss:0.62562
    [2]	validation_0-logloss:0.59157
    [3]	validation_0-logloss:0.56575
    [4]	validation_0-logloss:0.54151
    [5]	validation_0-logloss:0.52936
    [6]	validation_0-logloss:0.51211
    [7]	validation_0-logloss:0.49788
    [8]	validation_0-logloss:0.48490
    [9]	validation_0-logloss:0.47449
    [10]	validation_0-logloss:0.46532
    [11]	validation_0-logloss:0.45724
    [12]	validation_0-logloss:0.44891
    [13]	validation_0-logloss:0.43978
    [14]	validation_0-logloss:0.43419
    [15]	validation_0-logloss:0.42919
    [16]	validation_0-logloss:0.42539
    [17]	validation_0-logloss:0.42088
    [18]	validation_0-logloss:0.41754
    [19]	validation_0-logloss:0.41535
    [20]	validation_0-logloss:0.41287
    [21]	validation_0-logloss:0.41209
    [22]	validation_0-logloss:0.41021
    [23]	validation_0-logloss:0.40750
    [24]	validation_0-logloss:0.40531
    [25]	validation_0-logloss:0.40586
    [26]	validation_0-logloss:0.40524
    [27]	validation_0-logloss:0.40446
    [28]	validation_0-logloss:0.40470
    [29]	validation_0-logloss:0.40360
    [30]	validation_0-logloss:0.40275
    [31]	validation_0-logloss:0.40319
    [32]	validation_0-logloss:0.40250
    [33]	validation_0-logloss:0.40237
    [34]	validation_0-logloss:0.40277
    [35]	validation_0-logloss:0.40079
    [36]	validation_0-logloss:0.40072
    [37]	validation_0-logloss:0.40040
    [38]	validation_0-logloss:0.39997
    [39]	validation_0-logloss:0.39877
    [40]	validation_0-logloss:0.39871
    [41]	validation_0-logloss:0.39803
    [42]	validation_0-logloss:0.39828
    [43]	validation_0-logloss:0.39733
    [44]	validation_0-logloss:0.39731
    [45]	validation_0-logloss:0.39750
    [46]	validation_0-logloss:0.39733
    [47]	validation_0-logloss:0.39793
    [48]	validation_0-logloss:0.39845
    [49]	validation_0-logloss:0.39838
    [50]	validation_0-logloss:0.39876
    [51]	validation_0-logloss:0.39920
    [52]	validation_0-logloss:0.39937
    [53]	validation_0-logloss:0.39986
    [54]	validation_0-logloss:0.40084
    [55]	validation_0-logloss:0.40150
    [56]	validation_0-logloss:0.40214
    [57]	validation_0-logloss:0.40260
    [58]	validation_0-logloss:0.40376
    [59]	validation_0-logloss:0.40387
    [60]	validation_0-logloss:0.40466
    [61]	validation_0-logloss:0.40678
    [62]	validation_0-logloss:0.40854
    [63]	validation_0-logloss:0.40838
    [64]	validation_0-logloss:0.40893
    [65]	validation_0-logloss:0.40948
    [66]	validation_0-logloss:0.41010
    [67]	validation_0-logloss:0.41177
    [68]	validation_0-logloss:0.41233
    [69]	validation_0-logloss:0.41222
    [70]	validation_0-logloss:0.41369
    [71]	validation_0-logloss:0.41508
    [72]	validation_0-logloss:0.41508
    [73]	validation_0-logloss:0.41510
    [74]	validation_0-logloss:0.41524
    [75]	validation_0-logloss:0.41554
    [76]	validation_0-logloss:0.41629
    [77]	validation_0-logloss:0.41761
    [78]	validation_0-logloss:0.41770
    [79]	validation_0-logloss:0.41898
    [80]	validation_0-logloss:0.41924
    [81]	validation_0-logloss:0.41989
    [82]	validation_0-logloss:0.41955
    [83]	validation_0-logloss:0.41987
    [84]	validation_0-logloss:0.41982
    [85]	validation_0-logloss:0.42063
    [86]	validation_0-logloss:0.42154
    [87]	validation_0-logloss:0.42168
    [88]	validation_0-logloss:0.42148
    [89]	validation_0-logloss:0.42116
    [90]	validation_0-logloss:0.42076
    [91]	validation_0-logloss:0.42156
    [92]	validation_0-logloss:0.42253
    [93]	validation_0-logloss:0.42319
    [94]	validation_0-logloss:0.42493
    [95]	validation_0-logloss:0.42690
    [96]	validation_0-logloss:0.42840
    [97]	validation_0-logloss:0.42966
    [98]	validation_0-logloss:0.42966
    [99]	validation_0-logloss:0.42988
    [02:02:04] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64798
    [1]	validation_0-logloss:0.62447
    [2]	validation_0-logloss:0.59240
    [3]	validation_0-logloss:0.57041
    [4]	validation_0-logloss:0.54853
    [5]	validation_0-logloss:0.53739
    [6]	validation_0-logloss:0.52143
    [7]	validation_0-logloss:0.50802
    [8]	validation_0-logloss:0.49744
    [9]	validation_0-logloss:0.48440
    [10]	validation_0-logloss:0.47544
    [11]	validation_0-logloss:0.46780
    [12]	validation_0-logloss:0.46277
    [13]	validation_0-logloss:0.45812
    [14]	validation_0-logloss:0.45411
    [15]	validation_0-logloss:0.45071
    [16]	validation_0-logloss:0.44726
    [17]	validation_0-logloss:0.44353
    [18]	validation_0-logloss:0.43947
    [19]	validation_0-logloss:0.43680
    [20]	validation_0-logloss:0.43542
    [21]	validation_0-logloss:0.43339
    [22]	validation_0-logloss:0.43146
    [23]	validation_0-logloss:0.43176
    [24]	validation_0-logloss:0.43188
    [25]	validation_0-logloss:0.43131
    [26]	validation_0-logloss:0.43126
    [27]	validation_0-logloss:0.43329
    [28]	validation_0-logloss:0.43087
    [29]	validation_0-logloss:0.42981
    [30]	validation_0-logloss:0.43055
    [31]	validation_0-logloss:0.43062
    [32]	validation_0-logloss:0.43046
    [33]	validation_0-logloss:0.43164
    [34]	validation_0-logloss:0.43240
    [35]	validation_0-logloss:0.43298
    [36]	validation_0-logloss:0.43219
    [37]	validation_0-logloss:0.43200
    [38]	validation_0-logloss:0.43201
    [39]	validation_0-logloss:0.43338
    [40]	validation_0-logloss:0.43067
    [41]	validation_0-logloss:0.43016
    [42]	validation_0-logloss:0.42951
    [43]	validation_0-logloss:0.43034
    [44]	validation_0-logloss:0.43062
    [45]	validation_0-logloss:0.43171
    [46]	validation_0-logloss:0.43293
    [47]	validation_0-logloss:0.43372
    [48]	validation_0-logloss:0.43347
    [49]	validation_0-logloss:0.43391
    [50]	validation_0-logloss:0.43244
    [51]	validation_0-logloss:0.43282
    [52]	validation_0-logloss:0.43381
    [53]	validation_0-logloss:0.43354
    [54]	validation_0-logloss:0.43458
    [55]	validation_0-logloss:0.43411
    [56]	validation_0-logloss:0.43307
    [57]	validation_0-logloss:0.43277
    [58]	validation_0-logloss:0.43321
    [59]	validation_0-logloss:0.43394
    [60]	validation_0-logloss:0.43478
    [61]	validation_0-logloss:0.43581
    [62]	validation_0-logloss:0.43660
    [63]	validation_0-logloss:0.43877
    [64]	validation_0-logloss:0.43960
    [65]	validation_0-logloss:0.43916
    [66]	validation_0-logloss:0.43920
    [67]	validation_0-logloss:0.44038
    [68]	validation_0-logloss:0.44041
    [69]	validation_0-logloss:0.43906
    [70]	validation_0-logloss:0.43858
    [71]	validation_0-logloss:0.43762
    [72]	validation_0-logloss:0.43855
    [73]	validation_0-logloss:0.43911
    [74]	validation_0-logloss:0.44053
    [75]	validation_0-logloss:0.44158
    [76]	validation_0-logloss:0.44119
    [77]	validation_0-logloss:0.44096
    [78]	validation_0-logloss:0.44111
    [79]	validation_0-logloss:0.43947
    [80]	validation_0-logloss:0.44164
    [81]	validation_0-logloss:0.44111
    [82]	validation_0-logloss:0.44239
    [83]	validation_0-logloss:0.44131
    [84]	validation_0-logloss:0.44306
    [85]	validation_0-logloss:0.44279
    [86]	validation_0-logloss:0.44301
    [87]	validation_0-logloss:0.44326
    [88]	validation_0-logloss:0.44238
    [89]	validation_0-logloss:0.44216
    [90]	validation_0-logloss:0.44169
    [91]	validation_0-logloss:0.44146
    [92]	validation_0-logloss:0.44122
    [93]	validation_0-logloss:0.44137
    [94]	validation_0-logloss:0.44195
    [95]	validation_0-logloss:0.44174
    [96]	validation_0-logloss:0.44289
    [97]	validation_0-logloss:0.44353
    [98]	validation_0-logloss:0.44347
    [99]	validation_0-logloss:0.44333
    [02:02:05] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64759
    [1]	validation_0-logloss:0.62505
    [2]	validation_0-logloss:0.58963
    [3]	validation_0-logloss:0.56581
    [4]	validation_0-logloss:0.53995
    [5]	validation_0-logloss:0.52814
    [6]	validation_0-logloss:0.50962
    [7]	validation_0-logloss:0.49391
    [8]	validation_0-logloss:0.48217
    [9]	validation_0-logloss:0.47030
    [10]	validation_0-logloss:0.46104
    [11]	validation_0-logloss:0.45513
    [12]	validation_0-logloss:0.44705
    [13]	validation_0-logloss:0.43991
    [14]	validation_0-logloss:0.43529
    [15]	validation_0-logloss:0.42933
    [16]	validation_0-logloss:0.42601
    [17]	validation_0-logloss:0.42228
    [18]	validation_0-logloss:0.41874
    [19]	validation_0-logloss:0.41674
    [20]	validation_0-logloss:0.41543
    [21]	validation_0-logloss:0.41306
    [22]	validation_0-logloss:0.41152
    [23]	validation_0-logloss:0.40941
    [24]	validation_0-logloss:0.40803
    [25]	validation_0-logloss:0.40824
    [26]	validation_0-logloss:0.40664
    [27]	validation_0-logloss:0.40615
    [28]	validation_0-logloss:0.40714
    [29]	validation_0-logloss:0.40639
    [30]	validation_0-logloss:0.40475
    [31]	validation_0-logloss:0.40388
    [32]	validation_0-logloss:0.40473
    [33]	validation_0-logloss:0.40450
    [34]	validation_0-logloss:0.40493
    [35]	validation_0-logloss:0.40520
    [36]	validation_0-logloss:0.40541
    [37]	validation_0-logloss:0.40448
    [38]	validation_0-logloss:0.40437
    [39]	validation_0-logloss:0.40375
    [40]	validation_0-logloss:0.40444
    [41]	validation_0-logloss:0.40341
    [42]	validation_0-logloss:0.40469
    [43]	validation_0-logloss:0.40370
    [44]	validation_0-logloss:0.40357
    [45]	validation_0-logloss:0.40331
    [46]	validation_0-logloss:0.40354
    [47]	validation_0-logloss:0.40343
    [48]	validation_0-logloss:0.40337
    [49]	validation_0-logloss:0.40312
    [50]	validation_0-logloss:0.40348
    [51]	validation_0-logloss:0.40340
    [52]	validation_0-logloss:0.40387
    [53]	validation_0-logloss:0.40385
    [54]	validation_0-logloss:0.40376
    [55]	validation_0-logloss:0.40374
    [56]	validation_0-logloss:0.40370
    [57]	validation_0-logloss:0.40414
    [58]	validation_0-logloss:0.40454
    [59]	validation_0-logloss:0.40487
    [60]	validation_0-logloss:0.40611
    [61]	validation_0-logloss:0.40604
    [62]	validation_0-logloss:0.40573
    [63]	validation_0-logloss:0.40596
    [64]	validation_0-logloss:0.40680
    [65]	validation_0-logloss:0.40703
    [66]	validation_0-logloss:0.40731
    [67]	validation_0-logloss:0.40746
    [68]	validation_0-logloss:0.40778
    [69]	validation_0-logloss:0.40830
    [70]	validation_0-logloss:0.40832
    [71]	validation_0-logloss:0.40841
    [72]	validation_0-logloss:0.40899
    [73]	validation_0-logloss:0.40982
    [74]	validation_0-logloss:0.41072
    [75]	validation_0-logloss:0.41000
    [76]	validation_0-logloss:0.41000
    [77]	validation_0-logloss:0.41109
    [78]	validation_0-logloss:0.41162
    [79]	validation_0-logloss:0.41198
    [80]	validation_0-logloss:0.41310
    [81]	validation_0-logloss:0.41313
    [82]	validation_0-logloss:0.41327
    [83]	validation_0-logloss:0.41346
    [84]	validation_0-logloss:0.41429
    [85]	validation_0-logloss:0.41471
    [86]	validation_0-logloss:0.41533
    [87]	validation_0-logloss:0.41445
    [88]	validation_0-logloss:0.41444
    [89]	validation_0-logloss:0.41529
    [90]	validation_0-logloss:0.41530
    [91]	validation_0-logloss:0.41493
    [92]	validation_0-logloss:0.41484
    [93]	validation_0-logloss:0.41545
    [94]	validation_0-logloss:0.41587
    [95]	validation_0-logloss:0.41606
    [96]	validation_0-logloss:0.41604
    [97]	validation_0-logloss:0.41650
    [98]	validation_0-logloss:0.41640
    [99]	validation_0-logloss:0.41652
    [02:02:05] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64880
    [1]	validation_0-logloss:0.62657
    [2]	validation_0-logloss:0.59099
    [3]	validation_0-logloss:0.56584
    [4]	validation_0-logloss:0.54103
    [5]	validation_0-logloss:0.52936
    [6]	validation_0-logloss:0.51152
    [7]	validation_0-logloss:0.49611
    [8]	validation_0-logloss:0.48419
    [9]	validation_0-logloss:0.47253
    [10]	validation_0-logloss:0.46239
    [11]	validation_0-logloss:0.45361
    [12]	validation_0-logloss:0.44521
    [13]	validation_0-logloss:0.43718
    [14]	validation_0-logloss:0.43073
    [15]	validation_0-logloss:0.42597
    [16]	validation_0-logloss:0.42061
    [17]	validation_0-logloss:0.41681
    [18]	validation_0-logloss:0.41392
    [19]	validation_0-logloss:0.41100
    [20]	validation_0-logloss:0.40801
    [21]	validation_0-logloss:0.40585
    [22]	validation_0-logloss:0.40414
    [23]	validation_0-logloss:0.40184
    [24]	validation_0-logloss:0.39792
    [25]	validation_0-logloss:0.39776
    [26]	validation_0-logloss:0.39612
    [27]	validation_0-logloss:0.39496
    [28]	validation_0-logloss:0.39517
    [29]	validation_0-logloss:0.39442
    [30]	validation_0-logloss:0.39415
    [31]	validation_0-logloss:0.39304
    [32]	validation_0-logloss:0.39391
    [33]	validation_0-logloss:0.39205
    [34]	validation_0-logloss:0.39061
    [35]	validation_0-logloss:0.38998
    [36]	validation_0-logloss:0.38784
    [37]	validation_0-logloss:0.38738
    [38]	validation_0-logloss:0.38797
    [39]	validation_0-logloss:0.38833
    [40]	validation_0-logloss:0.38768
    [41]	validation_0-logloss:0.38686
    [42]	validation_0-logloss:0.38682
    [43]	validation_0-logloss:0.38748
    [44]	validation_0-logloss:0.38740
    [45]	validation_0-logloss:0.38791
    [46]	validation_0-logloss:0.38860
    [47]	validation_0-logloss:0.39016
    [48]	validation_0-logloss:0.38974
    [49]	validation_0-logloss:0.39018
    [50]	validation_0-logloss:0.39162
    [51]	validation_0-logloss:0.39087
    [52]	validation_0-logloss:0.39116
    [53]	validation_0-logloss:0.39122
    [54]	validation_0-logloss:0.39077
    [55]	validation_0-logloss:0.39078
    [56]	validation_0-logloss:0.39027
    [57]	validation_0-logloss:0.39193
    [58]	validation_0-logloss:0.39253
    [59]	validation_0-logloss:0.39285
    [60]	validation_0-logloss:0.39287
    [61]	validation_0-logloss:0.39200
    [62]	validation_0-logloss:0.39207
    [63]	validation_0-logloss:0.39201
    [64]	validation_0-logloss:0.39237
    [65]	validation_0-logloss:0.39274
    [66]	validation_0-logloss:0.39265
    [67]	validation_0-logloss:0.39504
    [68]	validation_0-logloss:0.39607
    [69]	validation_0-logloss:0.39648
    [70]	validation_0-logloss:0.39672
    [71]	validation_0-logloss:0.39684
    [72]	validation_0-logloss:0.39671
    [73]	validation_0-logloss:0.39687
    [74]	validation_0-logloss:0.39760
    [75]	validation_0-logloss:0.39859
    [76]	validation_0-logloss:0.39838
    [77]	validation_0-logloss:0.39901
    [78]	validation_0-logloss:0.39915
    [79]	validation_0-logloss:0.39852
    [80]	validation_0-logloss:0.39904
    [81]	validation_0-logloss:0.39955
    [82]	validation_0-logloss:0.39909
    [83]	validation_0-logloss:0.39941
    [84]	validation_0-logloss:0.39898
    [85]	validation_0-logloss:0.39965
    [86]	validation_0-logloss:0.39899
    [87]	validation_0-logloss:0.39947
    [88]	validation_0-logloss:0.40027
    [89]	validation_0-logloss:0.40089
    [90]	validation_0-logloss:0.40060
    [91]	validation_0-logloss:0.40071
    [92]	validation_0-logloss:0.40085
    [93]	validation_0-logloss:0.40053
    [94]	validation_0-logloss:0.40135
    [95]	validation_0-logloss:0.40230
    [96]	validation_0-logloss:0.40103
    [97]	validation_0-logloss:0.40238
    [98]	validation_0-logloss:0.40110
    [99]	validation_0-logloss:0.40096
    [02:02:06] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65121
    [1]	validation_0-logloss:0.63032
    [2]	validation_0-logloss:0.59769
    [3]	validation_0-logloss:0.57585
    [4]	validation_0-logloss:0.55334
    [5]	validation_0-logloss:0.54386
    [6]	validation_0-logloss:0.52750
    [7]	validation_0-logloss:0.51434
    [8]	validation_0-logloss:0.50350
    [9]	validation_0-logloss:0.49176
    [10]	validation_0-logloss:0.48505
    [11]	validation_0-logloss:0.47811
    [12]	validation_0-logloss:0.47236
    [13]	validation_0-logloss:0.46637
    [14]	validation_0-logloss:0.46310
    [15]	validation_0-logloss:0.45991
    [16]	validation_0-logloss:0.45819
    [17]	validation_0-logloss:0.45483
    [18]	validation_0-logloss:0.45136
    [19]	validation_0-logloss:0.44821
    [20]	validation_0-logloss:0.44610
    [21]	validation_0-logloss:0.44396
    [22]	validation_0-logloss:0.44181
    [23]	validation_0-logloss:0.44183
    [24]	validation_0-logloss:0.44142
    [25]	validation_0-logloss:0.44211
    [26]	validation_0-logloss:0.44242
    [27]	validation_0-logloss:0.44199
    [28]	validation_0-logloss:0.44228
    [29]	validation_0-logloss:0.44122
    [30]	validation_0-logloss:0.44250
    [31]	validation_0-logloss:0.44314
    [32]	validation_0-logloss:0.44266
    [33]	validation_0-logloss:0.44211
    [34]	validation_0-logloss:0.44216
    [35]	validation_0-logloss:0.44263
    [36]	validation_0-logloss:0.43998
    [37]	validation_0-logloss:0.44009
    [38]	validation_0-logloss:0.43959
    [39]	validation_0-logloss:0.43999
    [40]	validation_0-logloss:0.43756
    [41]	validation_0-logloss:0.43755
    [42]	validation_0-logloss:0.43827
    [43]	validation_0-logloss:0.43745
    [44]	validation_0-logloss:0.43835
    [45]	validation_0-logloss:0.44020
    [46]	validation_0-logloss:0.43996
    [47]	validation_0-logloss:0.44151
    [48]	validation_0-logloss:0.44242
    [49]	validation_0-logloss:0.44252
    [50]	validation_0-logloss:0.44356
    [51]	validation_0-logloss:0.44364
    [52]	validation_0-logloss:0.44242
    [53]	validation_0-logloss:0.44349
    [54]	validation_0-logloss:0.44370
    [55]	validation_0-logloss:0.44335
    [56]	validation_0-logloss:0.44380
    [57]	validation_0-logloss:0.44390
    [58]	validation_0-logloss:0.44499
    [59]	validation_0-logloss:0.44423
    [60]	validation_0-logloss:0.44615
    [61]	validation_0-logloss:0.44589
    [62]	validation_0-logloss:0.44637
    [63]	validation_0-logloss:0.44477
    [64]	validation_0-logloss:0.44586
    [65]	validation_0-logloss:0.44595
    [66]	validation_0-logloss:0.44538
    [67]	validation_0-logloss:0.44638
    [68]	validation_0-logloss:0.44742
    [69]	validation_0-logloss:0.44732
    [70]	validation_0-logloss:0.44748
    [71]	validation_0-logloss:0.44742
    [72]	validation_0-logloss:0.44731
    [73]	validation_0-logloss:0.44670
    [74]	validation_0-logloss:0.44543
    [75]	validation_0-logloss:0.44661
    [76]	validation_0-logloss:0.44586
    [77]	validation_0-logloss:0.44555
    [78]	validation_0-logloss:0.44616
    [79]	validation_0-logloss:0.44599
    [80]	validation_0-logloss:0.44654
    [81]	validation_0-logloss:0.44640
    [82]	validation_0-logloss:0.44721
    [83]	validation_0-logloss:0.44749
    [84]	validation_0-logloss:0.44726
    [85]	validation_0-logloss:0.44728
    [86]	validation_0-logloss:0.44620
    [87]	validation_0-logloss:0.44628
    [88]	validation_0-logloss:0.44591
    [89]	validation_0-logloss:0.44616
    [90]	validation_0-logloss:0.44578
    [91]	validation_0-logloss:0.44617
    [92]	validation_0-logloss:0.44626
    [93]	validation_0-logloss:0.44638
    [94]	validation_0-logloss:0.44638
    [95]	validation_0-logloss:0.44624
    [96]	validation_0-logloss:0.44671
    [97]	validation_0-logloss:0.44839
    [98]	validation_0-logloss:0.44788
    [99]	validation_0-logloss:0.44752
    [02:02:06] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64576
    [1]	validation_0-logloss:0.62373
    [2]	validation_0-logloss:0.58764
    [3]	validation_0-logloss:0.56404
    [4]	validation_0-logloss:0.53885
    [5]	validation_0-logloss:0.52693
    [6]	validation_0-logloss:0.50752
    [7]	validation_0-logloss:0.49114
    [8]	validation_0-logloss:0.47893
    [9]	validation_0-logloss:0.46725
    [10]	validation_0-logloss:0.45718
    [11]	validation_0-logloss:0.45045
    [12]	validation_0-logloss:0.44225
    [13]	validation_0-logloss:0.43424
    [14]	validation_0-logloss:0.42851
    [15]	validation_0-logloss:0.42445
    [16]	validation_0-logloss:0.42011
    [17]	validation_0-logloss:0.41643
    [18]	validation_0-logloss:0.41364
    [19]	validation_0-logloss:0.41188
    [20]	validation_0-logloss:0.41090
    [21]	validation_0-logloss:0.41010
    [22]	validation_0-logloss:0.41044
    [23]	validation_0-logloss:0.41047
    [24]	validation_0-logloss:0.41023
    [25]	validation_0-logloss:0.41101
    [26]	validation_0-logloss:0.41088
    [27]	validation_0-logloss:0.41117
    [28]	validation_0-logloss:0.41056
    [29]	validation_0-logloss:0.40951
    [30]	validation_0-logloss:0.40953
    [31]	validation_0-logloss:0.41212
    [32]	validation_0-logloss:0.41295
    [33]	validation_0-logloss:0.41142
    [34]	validation_0-logloss:0.41223
    [35]	validation_0-logloss:0.41345
    [36]	validation_0-logloss:0.41375
    [37]	validation_0-logloss:0.41520
    [38]	validation_0-logloss:0.41593
    [39]	validation_0-logloss:0.41667
    [40]	validation_0-logloss:0.41841
    [41]	validation_0-logloss:0.41849
    [42]	validation_0-logloss:0.41860
    [43]	validation_0-logloss:0.41919
    [44]	validation_0-logloss:0.42110
    [45]	validation_0-logloss:0.42339
    [46]	validation_0-logloss:0.42225
    [47]	validation_0-logloss:0.42296
    [48]	validation_0-logloss:0.42352
    [49]	validation_0-logloss:0.42371
    [50]	validation_0-logloss:0.42520
    [51]	validation_0-logloss:0.42767
    [52]	validation_0-logloss:0.42773
    [53]	validation_0-logloss:0.42780
    [54]	validation_0-logloss:0.42761
    [55]	validation_0-logloss:0.42824
    [56]	validation_0-logloss:0.42867
    [57]	validation_0-logloss:0.42920
    [58]	validation_0-logloss:0.43039
    [59]	validation_0-logloss:0.42956
    [60]	validation_0-logloss:0.43121
    [61]	validation_0-logloss:0.43194
    [62]	validation_0-logloss:0.43315
    [63]	validation_0-logloss:0.43380
    [64]	validation_0-logloss:0.43465
    [65]	validation_0-logloss:0.43498
    [66]	validation_0-logloss:0.43585
    [67]	validation_0-logloss:0.43881
    [68]	validation_0-logloss:0.44021
    [69]	validation_0-logloss:0.44068
    [70]	validation_0-logloss:0.44061
    [71]	validation_0-logloss:0.44050
    [72]	validation_0-logloss:0.44153
    [73]	validation_0-logloss:0.44286
    [74]	validation_0-logloss:0.44274
    [75]	validation_0-logloss:0.44511
    [76]	validation_0-logloss:0.44467
    [77]	validation_0-logloss:0.44492
    [78]	validation_0-logloss:0.44555
    [79]	validation_0-logloss:0.44569
    [80]	validation_0-logloss:0.44669
    [81]	validation_0-logloss:0.44767
    [82]	validation_0-logloss:0.44835
    [83]	validation_0-logloss:0.45046
    [84]	validation_0-logloss:0.45122
    [85]	validation_0-logloss:0.45132
    [86]	validation_0-logloss:0.45268
    [87]	validation_0-logloss:0.45289
    [88]	validation_0-logloss:0.45505
    [89]	validation_0-logloss:0.45614
    [90]	validation_0-logloss:0.45666
    [91]	validation_0-logloss:0.45783
    [92]	validation_0-logloss:0.45843
    [93]	validation_0-logloss:0.46005
    [94]	validation_0-logloss:0.46065
    [95]	validation_0-logloss:0.46053
    [96]	validation_0-logloss:0.46120
    [97]	validation_0-logloss:0.46239
    [98]	validation_0-logloss:0.46288
    [99]	validation_0-logloss:0.46327
    [02:02:07] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64861
    [1]	validation_0-logloss:0.62738
    [2]	validation_0-logloss:0.59310
    [3]	validation_0-logloss:0.56721
    [4]	validation_0-logloss:0.54303
    [5]	validation_0-logloss:0.53042
    [6]	validation_0-logloss:0.51184
    [7]	validation_0-logloss:0.49735
    [8]	validation_0-logloss:0.48449
    [9]	validation_0-logloss:0.47466
    [10]	validation_0-logloss:0.46628
    [11]	validation_0-logloss:0.45829
    [12]	validation_0-logloss:0.44902
    [13]	validation_0-logloss:0.44219
    [14]	validation_0-logloss:0.43524
    [15]	validation_0-logloss:0.43013
    [16]	validation_0-logloss:0.42528
    [17]	validation_0-logloss:0.42074
    [18]	validation_0-logloss:0.41772
    [19]	validation_0-logloss:0.41551
    [20]	validation_0-logloss:0.41297
    [21]	validation_0-logloss:0.41197
    [22]	validation_0-logloss:0.41160
    [23]	validation_0-logloss:0.40992
    [24]	validation_0-logloss:0.40843
    [25]	validation_0-logloss:0.41019
    [26]	validation_0-logloss:0.40968
    [27]	validation_0-logloss:0.40917
    [28]	validation_0-logloss:0.40793
    [29]	validation_0-logloss:0.40674
    [30]	validation_0-logloss:0.40807
    [31]	validation_0-logloss:0.40822
    [32]	validation_0-logloss:0.40905
    [33]	validation_0-logloss:0.40993
    [34]	validation_0-logloss:0.41022
    [35]	validation_0-logloss:0.41020
    [36]	validation_0-logloss:0.41022
    [37]	validation_0-logloss:0.41069
    [38]	validation_0-logloss:0.41089
    [39]	validation_0-logloss:0.41248
    [40]	validation_0-logloss:0.41203
    [41]	validation_0-logloss:0.41176
    [42]	validation_0-logloss:0.41148
    [43]	validation_0-logloss:0.41158
    [44]	validation_0-logloss:0.41263
    [45]	validation_0-logloss:0.41340
    [46]	validation_0-logloss:0.41341
    [47]	validation_0-logloss:0.41485
    [48]	validation_0-logloss:0.41581
    [49]	validation_0-logloss:0.41485
    [50]	validation_0-logloss:0.41526
    [51]	validation_0-logloss:0.41628
    [52]	validation_0-logloss:0.41601
    [53]	validation_0-logloss:0.41719
    [54]	validation_0-logloss:0.41852
    [55]	validation_0-logloss:0.41846
    [56]	validation_0-logloss:0.41931
    [57]	validation_0-logloss:0.41912
    [58]	validation_0-logloss:0.41859
    [59]	validation_0-logloss:0.41738
    [60]	validation_0-logloss:0.41812
    [61]	validation_0-logloss:0.42005
    [62]	validation_0-logloss:0.42157
    [63]	validation_0-logloss:0.42234
    [64]	validation_0-logloss:0.42165
    [65]	validation_0-logloss:0.42242
    [66]	validation_0-logloss:0.42386
    [67]	validation_0-logloss:0.42597
    [68]	validation_0-logloss:0.42723
    [69]	validation_0-logloss:0.42754
    [70]	validation_0-logloss:0.42744
    [71]	validation_0-logloss:0.42783
    [72]	validation_0-logloss:0.42827
    [73]	validation_0-logloss:0.42877
    [74]	validation_0-logloss:0.42873
    [75]	validation_0-logloss:0.43006
    [76]	validation_0-logloss:0.43076
    [77]	validation_0-logloss:0.43155
    [78]	validation_0-logloss:0.43247
    [79]	validation_0-logloss:0.43310
    [80]	validation_0-logloss:0.43308
    [81]	validation_0-logloss:0.43407
    [82]	validation_0-logloss:0.43458
    [83]	validation_0-logloss:0.43531
    [84]	validation_0-logloss:0.43478
    [85]	validation_0-logloss:0.43470
    [86]	validation_0-logloss:0.43683
    [87]	validation_0-logloss:0.43743
    [88]	validation_0-logloss:0.43760
    [89]	validation_0-logloss:0.43882
    [90]	validation_0-logloss:0.43932
    [91]	validation_0-logloss:0.43972
    [92]	validation_0-logloss:0.43988
    [93]	validation_0-logloss:0.44018
    [94]	validation_0-logloss:0.44186
    [95]	validation_0-logloss:0.44035
    [96]	validation_0-logloss:0.44000
    [97]	validation_0-logloss:0.43975
    [98]	validation_0-logloss:0.44092
    [99]	validation_0-logloss:0.44116
    [02:02:08] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64846
    [1]	validation_0-logloss:0.62415
    [2]	validation_0-logloss:0.59180
    [3]	validation_0-logloss:0.56980
    [4]	validation_0-logloss:0.54817
    [5]	validation_0-logloss:0.53605
    [6]	validation_0-logloss:0.52017
    [7]	validation_0-logloss:0.50574
    [8]	validation_0-logloss:0.49505
    [9]	validation_0-logloss:0.48059
    [10]	validation_0-logloss:0.47255
    [11]	validation_0-logloss:0.46417
    [12]	validation_0-logloss:0.45976
    [13]	validation_0-logloss:0.45544
    [14]	validation_0-logloss:0.45030
    [15]	validation_0-logloss:0.44825
    [16]	validation_0-logloss:0.44459
    [17]	validation_0-logloss:0.44064
    [18]	validation_0-logloss:0.43612
    [19]	validation_0-logloss:0.43417
    [20]	validation_0-logloss:0.43318
    [21]	validation_0-logloss:0.43279
    [22]	validation_0-logloss:0.42874
    [23]	validation_0-logloss:0.42921
    [24]	validation_0-logloss:0.42950
    [25]	validation_0-logloss:0.43148
    [26]	validation_0-logloss:0.43267
    [27]	validation_0-logloss:0.43328
    [28]	validation_0-logloss:0.43200
    [29]	validation_0-logloss:0.43090
    [30]	validation_0-logloss:0.43145
    [31]	validation_0-logloss:0.43223
    [32]	validation_0-logloss:0.43349
    [33]	validation_0-logloss:0.43292
    [34]	validation_0-logloss:0.43248
    [35]	validation_0-logloss:0.43225
    [36]	validation_0-logloss:0.43259
    [37]	validation_0-logloss:0.43382
    [38]	validation_0-logloss:0.43510
    [39]	validation_0-logloss:0.43757
    [40]	validation_0-logloss:0.43820
    [41]	validation_0-logloss:0.43800
    [42]	validation_0-logloss:0.43958
    [43]	validation_0-logloss:0.44042
    [44]	validation_0-logloss:0.43948
    [45]	validation_0-logloss:0.43994
    [46]	validation_0-logloss:0.43956
    [47]	validation_0-logloss:0.44130
    [48]	validation_0-logloss:0.44078
    [49]	validation_0-logloss:0.44159
    [50]	validation_0-logloss:0.44291
    [51]	validation_0-logloss:0.44341
    [52]	validation_0-logloss:0.44247
    [53]	validation_0-logloss:0.44343
    [54]	validation_0-logloss:0.44257
    [55]	validation_0-logloss:0.44328
    [56]	validation_0-logloss:0.44300
    [57]	validation_0-logloss:0.44388
    [58]	validation_0-logloss:0.44460
    [59]	validation_0-logloss:0.44529
    [60]	validation_0-logloss:0.44555
    [61]	validation_0-logloss:0.44620
    [62]	validation_0-logloss:0.44779
    [63]	validation_0-logloss:0.44840
    [64]	validation_0-logloss:0.44988
    [65]	validation_0-logloss:0.44918
    [66]	validation_0-logloss:0.44924
    [67]	validation_0-logloss:0.45040
    [68]	validation_0-logloss:0.45131
    [69]	validation_0-logloss:0.45087
    [70]	validation_0-logloss:0.45097
    [71]	validation_0-logloss:0.45133
    [72]	validation_0-logloss:0.45104
    [73]	validation_0-logloss:0.45123
    [74]	validation_0-logloss:0.45281
    [75]	validation_0-logloss:0.45391
    [76]	validation_0-logloss:0.45487
    [77]	validation_0-logloss:0.45591
    [78]	validation_0-logloss:0.45593
    [79]	validation_0-logloss:0.45708
    [80]	validation_0-logloss:0.45803
    [81]	validation_0-logloss:0.45859
    [82]	validation_0-logloss:0.45876
    [83]	validation_0-logloss:0.45789
    [84]	validation_0-logloss:0.45876
    [85]	validation_0-logloss:0.45799
    [86]	validation_0-logloss:0.45758
    [87]	validation_0-logloss:0.45823
    [88]	validation_0-logloss:0.45712
    [89]	validation_0-logloss:0.45717
    [90]	validation_0-logloss:0.45694
    [91]	validation_0-logloss:0.45750
    [92]	validation_0-logloss:0.45851
    [93]	validation_0-logloss:0.45848
    [94]	validation_0-logloss:0.46001
    [95]	validation_0-logloss:0.46045
    [96]	validation_0-logloss:0.46108
    [97]	validation_0-logloss:0.46191
    [98]	validation_0-logloss:0.46154
    [99]	validation_0-logloss:0.46063
    [02:02:08] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64692
    [1]	validation_0-logloss:0.62468
    [2]	validation_0-logloss:0.58941
    [3]	validation_0-logloss:0.56552
    [4]	validation_0-logloss:0.53954
    [5]	validation_0-logloss:0.52875
    [6]	validation_0-logloss:0.51080
    [7]	validation_0-logloss:0.49498
    [8]	validation_0-logloss:0.48372
    [9]	validation_0-logloss:0.47126
    [10]	validation_0-logloss:0.46123
    [11]	validation_0-logloss:0.45268
    [12]	validation_0-logloss:0.44556
    [13]	validation_0-logloss:0.43786
    [14]	validation_0-logloss:0.43247
    [15]	validation_0-logloss:0.42795
    [16]	validation_0-logloss:0.42411
    [17]	validation_0-logloss:0.42035
    [18]	validation_0-logloss:0.41647
    [19]	validation_0-logloss:0.41415
    [20]	validation_0-logloss:0.41171
    [21]	validation_0-logloss:0.41024
    [22]	validation_0-logloss:0.40859
    [23]	validation_0-logloss:0.40559
    [24]	validation_0-logloss:0.40419
    [25]	validation_0-logloss:0.40481
    [26]	validation_0-logloss:0.40342
    [27]	validation_0-logloss:0.40356
    [28]	validation_0-logloss:0.40433
    [29]	validation_0-logloss:0.40353
    [30]	validation_0-logloss:0.40294
    [31]	validation_0-logloss:0.40302
    [32]	validation_0-logloss:0.40311
    [33]	validation_0-logloss:0.40303
    [34]	validation_0-logloss:0.40328
    [35]	validation_0-logloss:0.40275
    [36]	validation_0-logloss:0.40226
    [37]	validation_0-logloss:0.40286
    [38]	validation_0-logloss:0.40269
    [39]	validation_0-logloss:0.40169
    [40]	validation_0-logloss:0.40164
    [41]	validation_0-logloss:0.40166
    [42]	validation_0-logloss:0.40176
    [43]	validation_0-logloss:0.40143
    [44]	validation_0-logloss:0.40132
    [45]	validation_0-logloss:0.40159
    [46]	validation_0-logloss:0.40250
    [47]	validation_0-logloss:0.40321
    [48]	validation_0-logloss:0.40296
    [49]	validation_0-logloss:0.40381
    [50]	validation_0-logloss:0.40284
    [51]	validation_0-logloss:0.40319
    [52]	validation_0-logloss:0.40422
    [53]	validation_0-logloss:0.40489
    [54]	validation_0-logloss:0.40585
    [55]	validation_0-logloss:0.40704
    [56]	validation_0-logloss:0.40578
    [57]	validation_0-logloss:0.40608
    [58]	validation_0-logloss:0.40652
    [59]	validation_0-logloss:0.40700
    [60]	validation_0-logloss:0.40791
    [61]	validation_0-logloss:0.40823
    [62]	validation_0-logloss:0.40918
    [63]	validation_0-logloss:0.41050
    [64]	validation_0-logloss:0.41099
    [65]	validation_0-logloss:0.41150
    [66]	validation_0-logloss:0.41292
    [67]	validation_0-logloss:0.41371
    [68]	validation_0-logloss:0.41444
    [69]	validation_0-logloss:0.41434
    [70]	validation_0-logloss:0.41415
    [71]	validation_0-logloss:0.41380
    [72]	validation_0-logloss:0.41473
    [73]	validation_0-logloss:0.41511
    [74]	validation_0-logloss:0.41557
    [75]	validation_0-logloss:0.41619
    [76]	validation_0-logloss:0.41567
    [77]	validation_0-logloss:0.41659
    [78]	validation_0-logloss:0.41646
    [79]	validation_0-logloss:0.41536
    [80]	validation_0-logloss:0.41586
    [81]	validation_0-logloss:0.41619
    [82]	validation_0-logloss:0.41662
    [83]	validation_0-logloss:0.41681
    [84]	validation_0-logloss:0.41846
    [85]	validation_0-logloss:0.41848
    [86]	validation_0-logloss:0.41788
    [87]	validation_0-logloss:0.41909
    [88]	validation_0-logloss:0.41861
    [89]	validation_0-logloss:0.41958
    [90]	validation_0-logloss:0.41924
    [91]	validation_0-logloss:0.41983
    [92]	validation_0-logloss:0.42004
    [93]	validation_0-logloss:0.42091
    [94]	validation_0-logloss:0.42218
    [95]	validation_0-logloss:0.42222
    [96]	validation_0-logloss:0.42251
    [97]	validation_0-logloss:0.42228
    [98]	validation_0-logloss:0.42178
    [99]	validation_0-logloss:0.42226
    [02:02:09] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64836
    [1]	validation_0-logloss:0.62493
    [2]	validation_0-logloss:0.58923
    [3]	validation_0-logloss:0.56429
    [4]	validation_0-logloss:0.53943
    [5]	validation_0-logloss:0.52718
    [6]	validation_0-logloss:0.50926
    [7]	validation_0-logloss:0.49386
    [8]	validation_0-logloss:0.48204
    [9]	validation_0-logloss:0.47001
    [10]	validation_0-logloss:0.45996
    [11]	validation_0-logloss:0.45179
    [12]	validation_0-logloss:0.44373
    [13]	validation_0-logloss:0.43396
    [14]	validation_0-logloss:0.42715
    [15]	validation_0-logloss:0.42267
    [16]	validation_0-logloss:0.41739
    [17]	validation_0-logloss:0.41359
    [18]	validation_0-logloss:0.41059
    [19]	validation_0-logloss:0.40879
    [20]	validation_0-logloss:0.40572
    [21]	validation_0-logloss:0.40413
    [22]	validation_0-logloss:0.40222
    [23]	validation_0-logloss:0.39961
    [24]	validation_0-logloss:0.39719
    [25]	validation_0-logloss:0.39710
    [26]	validation_0-logloss:0.39481
    [27]	validation_0-logloss:0.39544
    [28]	validation_0-logloss:0.39521
    [29]	validation_0-logloss:0.39440
    [30]	validation_0-logloss:0.39233
    [31]	validation_0-logloss:0.39214
    [32]	validation_0-logloss:0.39273
    [33]	validation_0-logloss:0.39136
    [34]	validation_0-logloss:0.39144
    [35]	validation_0-logloss:0.39120
    [36]	validation_0-logloss:0.39069
    [37]	validation_0-logloss:0.38971
    [38]	validation_0-logloss:0.39002
    [39]	validation_0-logloss:0.39002
    [40]	validation_0-logloss:0.38951
    [41]	validation_0-logloss:0.38924
    [42]	validation_0-logloss:0.39136
    [43]	validation_0-logloss:0.39210
    [44]	validation_0-logloss:0.39129
    [45]	validation_0-logloss:0.39184
    [46]	validation_0-logloss:0.39136
    [47]	validation_0-logloss:0.39073
    [48]	validation_0-logloss:0.39104
    [49]	validation_0-logloss:0.39105
    [50]	validation_0-logloss:0.39105
    [51]	validation_0-logloss:0.39233
    [52]	validation_0-logloss:0.39404
    [53]	validation_0-logloss:0.39446
    [54]	validation_0-logloss:0.39432
    [55]	validation_0-logloss:0.39442
    [56]	validation_0-logloss:0.39415
    [57]	validation_0-logloss:0.39489
    [58]	validation_0-logloss:0.39567
    [59]	validation_0-logloss:0.39714
    [60]	validation_0-logloss:0.39768
    [61]	validation_0-logloss:0.39815
    [62]	validation_0-logloss:0.39896
    [63]	validation_0-logloss:0.39889
    [64]	validation_0-logloss:0.39897
    [65]	validation_0-logloss:0.39883
    [66]	validation_0-logloss:0.39904
    [67]	validation_0-logloss:0.39920
    [68]	validation_0-logloss:0.40011
    [69]	validation_0-logloss:0.40007
    [70]	validation_0-logloss:0.39994
    [71]	validation_0-logloss:0.39929
    [72]	validation_0-logloss:0.39880
    [73]	validation_0-logloss:0.39964
    [74]	validation_0-logloss:0.39954
    [75]	validation_0-logloss:0.40110
    [76]	validation_0-logloss:0.40104
    [77]	validation_0-logloss:0.40237
    [78]	validation_0-logloss:0.40281
    [79]	validation_0-logloss:0.40338
    [80]	validation_0-logloss:0.40303
    [81]	validation_0-logloss:0.40367
    [82]	validation_0-logloss:0.40532
    [83]	validation_0-logloss:0.40517
    [84]	validation_0-logloss:0.40614
    [85]	validation_0-logloss:0.40646
    [86]	validation_0-logloss:0.40616
    [87]	validation_0-logloss:0.40728
    [88]	validation_0-logloss:0.40638
    [89]	validation_0-logloss:0.40682
    [90]	validation_0-logloss:0.40648
    [91]	validation_0-logloss:0.40685
    [92]	validation_0-logloss:0.40725
    [93]	validation_0-logloss:0.40791
    [94]	validation_0-logloss:0.40826
    [95]	validation_0-logloss:0.40821
    [96]	validation_0-logloss:0.40919
    [97]	validation_0-logloss:0.41040
    [98]	validation_0-logloss:0.41030
    [99]	validation_0-logloss:0.40924
    [02:02:10] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.65121
    [1]	validation_0-logloss:0.63037
    [2]	validation_0-logloss:0.59779
    [3]	validation_0-logloss:0.57630
    [4]	validation_0-logloss:0.55378
    [5]	validation_0-logloss:0.54459
    [6]	validation_0-logloss:0.52816
    [7]	validation_0-logloss:0.51590
    [8]	validation_0-logloss:0.50509
    [9]	validation_0-logloss:0.49338
    [10]	validation_0-logloss:0.48529
    [11]	validation_0-logloss:0.47873
    [12]	validation_0-logloss:0.47236
    [13]	validation_0-logloss:0.46598
    [14]	validation_0-logloss:0.46291
    [15]	validation_0-logloss:0.45967
    [16]	validation_0-logloss:0.45785
    [17]	validation_0-logloss:0.45435
    [18]	validation_0-logloss:0.45049
    [19]	validation_0-logloss:0.44933
    [20]	validation_0-logloss:0.44695
    [21]	validation_0-logloss:0.44420
    [22]	validation_0-logloss:0.44310
    [23]	validation_0-logloss:0.44297
    [24]	validation_0-logloss:0.44299
    [25]	validation_0-logloss:0.44467
    [26]	validation_0-logloss:0.44470
    [27]	validation_0-logloss:0.44331
    [28]	validation_0-logloss:0.44259
    [29]	validation_0-logloss:0.44160
    [30]	validation_0-logloss:0.44384
    [31]	validation_0-logloss:0.44499
    [32]	validation_0-logloss:0.44434
    [33]	validation_0-logloss:0.44387
    [34]	validation_0-logloss:0.44308
    [35]	validation_0-logloss:0.44316
    [36]	validation_0-logloss:0.44174
    [37]	validation_0-logloss:0.44112
    [38]	validation_0-logloss:0.44051
    [39]	validation_0-logloss:0.44100
    [40]	validation_0-logloss:0.43922
    [41]	validation_0-logloss:0.43946
    [42]	validation_0-logloss:0.43934
    [43]	validation_0-logloss:0.43893
    [44]	validation_0-logloss:0.43921
    [45]	validation_0-logloss:0.44066
    [46]	validation_0-logloss:0.44058
    [47]	validation_0-logloss:0.44292
    [48]	validation_0-logloss:0.44440
    [49]	validation_0-logloss:0.44472
    [50]	validation_0-logloss:0.44511
    [51]	validation_0-logloss:0.44692
    [52]	validation_0-logloss:0.44606
    [53]	validation_0-logloss:0.44766
    [54]	validation_0-logloss:0.44648
    [55]	validation_0-logloss:0.44637
    [56]	validation_0-logloss:0.44700
    [57]	validation_0-logloss:0.44867
    [58]	validation_0-logloss:0.44931
    [59]	validation_0-logloss:0.45019
    [60]	validation_0-logloss:0.45113
    [61]	validation_0-logloss:0.45139
    [62]	validation_0-logloss:0.45311
    [63]	validation_0-logloss:0.45346
    [64]	validation_0-logloss:0.45442
    [65]	validation_0-logloss:0.45502
    [66]	validation_0-logloss:0.45391
    [67]	validation_0-logloss:0.45487
    [68]	validation_0-logloss:0.45720
    [69]	validation_0-logloss:0.45739
    [70]	validation_0-logloss:0.45656
    [71]	validation_0-logloss:0.45569
    [72]	validation_0-logloss:0.45501
    [73]	validation_0-logloss:0.45436
    [74]	validation_0-logloss:0.45646
    [75]	validation_0-logloss:0.45711
    [76]	validation_0-logloss:0.45714
    [77]	validation_0-logloss:0.45791
    [78]	validation_0-logloss:0.45967
    [79]	validation_0-logloss:0.45905
    [80]	validation_0-logloss:0.45964
    [81]	validation_0-logloss:0.45950
    [82]	validation_0-logloss:0.46118
    [83]	validation_0-logloss:0.46169
    [84]	validation_0-logloss:0.46144
    [85]	validation_0-logloss:0.46064
    [86]	validation_0-logloss:0.46154
    [87]	validation_0-logloss:0.46119
    [88]	validation_0-logloss:0.46090
    [89]	validation_0-logloss:0.45993
    [90]	validation_0-logloss:0.45922
    [91]	validation_0-logloss:0.45969
    [92]	validation_0-logloss:0.45915
    [93]	validation_0-logloss:0.45922
    [94]	validation_0-logloss:0.45990
    [95]	validation_0-logloss:0.46041
    [96]	validation_0-logloss:0.46105
    [97]	validation_0-logloss:0.46199
    [98]	validation_0-logloss:0.46256
    [99]	validation_0-logloss:0.46165
    [02:02:10] WARNING: ../src/learner.cc:576: 
    Parameters: { "n_esimators" } might not be used.
    
      This could be a false alarm, with some parameters getting used by language bindings but
      then being mistakenly passed down to XGBoost core, or some parameter actually being used
      but getting flagged wrongly here. Please open an issue if you find any such cases.
    
    
    [0]	validation_0-logloss:0.64594
    [1]	validation_0-logloss:0.62194
    [2]	validation_0-logloss:0.58630
    [3]	validation_0-logloss:0.56021
    [4]	validation_0-logloss:0.53646
    [5]	validation_0-logloss:0.52380
    [6]	validation_0-logloss:0.50337
    [7]	validation_0-logloss:0.48574
    [8]	validation_0-logloss:0.47236
    [9]	validation_0-logloss:0.45841
    [10]	validation_0-logloss:0.44762
    [11]	validation_0-logloss:0.43829
    [12]	validation_0-logloss:0.43170
    [13]	validation_0-logloss:0.42711
    [14]	validation_0-logloss:0.42148
    [15]	validation_0-logloss:0.41714
    [16]	validation_0-logloss:0.41491
    [17]	validation_0-logloss:0.41119
    [18]	validation_0-logloss:0.40741
    [19]	validation_0-logloss:0.40667
    [20]	validation_0-logloss:0.40567
    [21]	validation_0-logloss:0.40466
    [22]	validation_0-logloss:0.40433
    [23]	validation_0-logloss:0.40313
    [24]	validation_0-logloss:0.40146
    [25]	validation_0-logloss:0.40090
    [26]	validation_0-logloss:0.39949
    [27]	validation_0-logloss:0.39997
    [28]	validation_0-logloss:0.39988
    [29]	validation_0-logloss:0.39913
    [30]	validation_0-logloss:0.39927
    [31]	validation_0-logloss:0.39849
    [32]	validation_0-logloss:0.39927
    [33]	validation_0-logloss:0.39857
    [34]	validation_0-logloss:0.39804
    [35]	validation_0-logloss:0.39813
    [36]	validation_0-logloss:0.39861
    [37]	validation_0-logloss:0.39930
    [38]	validation_0-logloss:0.40019
    [39]	validation_0-logloss:0.40078
    [40]	validation_0-logloss:0.40142
    [41]	validation_0-logloss:0.40116
    [42]	validation_0-logloss:0.40142
    [43]	validation_0-logloss:0.40204
    [44]	validation_0-logloss:0.40209
    [45]	validation_0-logloss:0.40096
    [46]	validation_0-logloss:0.40093
    [47]	validation_0-logloss:0.40224
    [48]	validation_0-logloss:0.40288
    [49]	validation_0-logloss:0.40221
    [50]	validation_0-logloss:0.40375
    [51]	validation_0-logloss:0.40359
    [52]	validation_0-logloss:0.40289
    [53]	validation_0-logloss:0.40342
    [54]	validation_0-logloss:0.40528
    [55]	validation_0-logloss:0.40486
    [56]	validation_0-logloss:0.40504
    [57]	validation_0-logloss:0.40491
    [58]	validation_0-logloss:0.40474
    [59]	validation_0-logloss:0.40463
    [60]	validation_0-logloss:0.40464
    [61]	validation_0-logloss:0.40423
    [62]	validation_0-logloss:0.40526
    [63]	validation_0-logloss:0.40588
    [64]	validation_0-logloss:0.40586
    [65]	validation_0-logloss:0.40567
    [66]	validation_0-logloss:0.40618
    [67]	validation_0-logloss:0.40675
    [68]	validation_0-logloss:0.40780
    [69]	validation_0-logloss:0.40689
    [70]	validation_0-logloss:0.40700
    [71]	validation_0-logloss:0.40704
    [72]	validation_0-logloss:0.40708
    [73]	validation_0-logloss:0.40773
    [74]	validation_0-logloss:0.40858
    [75]	validation_0-logloss:0.40874
    [76]	validation_0-logloss:0.40801
    [77]	validation_0-logloss:0.40806
    [78]	validation_0-logloss:0.40934
    [79]	validation_0-logloss:0.40952
    [80]	validation_0-logloss:0.40920
    [81]	validation_0-logloss:0.41025
    [82]	validation_0-logloss:0.41074
    [83]	validation_0-logloss:0.40991
    [84]	validation_0-logloss:0.41223
    [85]	validation_0-logloss:0.41283
    [86]	validation_0-logloss:0.41303
    [87]	validation_0-logloss:0.41379
    [88]	validation_0-logloss:0.41389
    [89]	validation_0-logloss:0.41221
    [90]	validation_0-logloss:0.41239
    [91]	validation_0-logloss:0.41262
    [92]	validation_0-logloss:0.41284
    [93]	validation_0-logloss:0.41241
    [94]	validation_0-logloss:0.41280
    [95]	validation_0-logloss:0.41280
    [96]	validation_0-logloss:0.41290
    [97]	validation_0-logloss:0.41286
    [98]	validation_0-logloss:0.41448
    [99]	validation_0-logloss:0.41551



```python
# .best_estimator 하면 따로 best params 출력해서 넣는거 안해두댐
```


```python
gridcv.best_params_
```




    {'colsample_bytree': 0.75, 'max_depth': 7, 'min_child_weight': 1}




```python
xgb_model_p=XGBClassifier(colsample_bytree=0.75, max_depth=7, min_child_weight= 1)
xgb_model_p.fit(X_train1,y_train1,early_stopping_rounds=100,eval_metric='logloss',eval_set=evals1,verbose=True)

xgb_pred=xgb_model_p.predict(X_test1)
xgb_pred_prob=xgb_model_p.predict_proba(X_test1)[:,1]
```

    [0]	validation_0-logloss:0.57304
    [1]	validation_0-logloss:0.49871
    [2]	validation_0-logloss:0.45503
    [3]	validation_0-logloss:0.42428
    [4]	validation_0-logloss:0.41152
    [5]	validation_0-logloss:0.40544
    [6]	validation_0-logloss:0.39781
    [7]	validation_0-logloss:0.39946
    [8]	validation_0-logloss:0.39790
    [9]	validation_0-logloss:0.39670
    [10]	validation_0-logloss:0.39826
    [11]	validation_0-logloss:0.40212
    [12]	validation_0-logloss:0.39754
    [13]	validation_0-logloss:0.40113
    [14]	validation_0-logloss:0.40052
    [15]	validation_0-logloss:0.40232
    [16]	validation_0-logloss:0.40407
    [17]	validation_0-logloss:0.40447
    [18]	validation_0-logloss:0.40808
    [19]	validation_0-logloss:0.40866
    [20]	validation_0-logloss:0.40899
    [21]	validation_0-logloss:0.41012
    [22]	validation_0-logloss:0.41346
    [23]	validation_0-logloss:0.41882
    [24]	validation_0-logloss:0.42253
    [25]	validation_0-logloss:0.42696
    [26]	validation_0-logloss:0.43466
    [27]	validation_0-logloss:0.43857
    [28]	validation_0-logloss:0.43851
    [29]	validation_0-logloss:0.43955
    [30]	validation_0-logloss:0.44221
    [31]	validation_0-logloss:0.44367
    [32]	validation_0-logloss:0.44819
    [33]	validation_0-logloss:0.44966
    [34]	validation_0-logloss:0.45030
    [35]	validation_0-logloss:0.45421
    [36]	validation_0-logloss:0.45418
    [37]	validation_0-logloss:0.45911
    [38]	validation_0-logloss:0.46078
    [39]	validation_0-logloss:0.45948
    [40]	validation_0-logloss:0.46270
    [41]	validation_0-logloss:0.46458
    [42]	validation_0-logloss:0.46364
    [43]	validation_0-logloss:0.46590
    [44]	validation_0-logloss:0.46733
    [45]	validation_0-logloss:0.46867
    [46]	validation_0-logloss:0.47224
    [47]	validation_0-logloss:0.47260
    [48]	validation_0-logloss:0.47537
    [49]	validation_0-logloss:0.47380
    [50]	validation_0-logloss:0.47435
    [51]	validation_0-logloss:0.47611
    [52]	validation_0-logloss:0.47659
    [53]	validation_0-logloss:0.48056
    [54]	validation_0-logloss:0.48551
    [55]	validation_0-logloss:0.48267
    [56]	validation_0-logloss:0.48297
    [57]	validation_0-logloss:0.48490
    [58]	validation_0-logloss:0.48586
    [59]	validation_0-logloss:0.48584
    [60]	validation_0-logloss:0.48879
    [61]	validation_0-logloss:0.49017
    [62]	validation_0-logloss:0.49243
    [63]	validation_0-logloss:0.49554
    [64]	validation_0-logloss:0.49713
    [65]	validation_0-logloss:0.49861
    [66]	validation_0-logloss:0.50119
    [67]	validation_0-logloss:0.50113
    [68]	validation_0-logloss:0.50353
    [69]	validation_0-logloss:0.50424
    [70]	validation_0-logloss:0.50426
    [71]	validation_0-logloss:0.50374
    [72]	validation_0-logloss:0.50409
    [73]	validation_0-logloss:0.50431
    [74]	validation_0-logloss:0.50534
    [75]	validation_0-logloss:0.50251
    [76]	validation_0-logloss:0.50360
    [77]	validation_0-logloss:0.50765
    [78]	validation_0-logloss:0.50858
    [79]	validation_0-logloss:0.51001
    [80]	validation_0-logloss:0.51253
    [81]	validation_0-logloss:0.51345
    [82]	validation_0-logloss:0.51074
    [83]	validation_0-logloss:0.51317
    [84]	validation_0-logloss:0.51307
    [85]	validation_0-logloss:0.51116
    [86]	validation_0-logloss:0.51341
    [87]	validation_0-logloss:0.51457
    [88]	validation_0-logloss:0.51578
    [89]	validation_0-logloss:0.51582
    [90]	validation_0-logloss:0.51787
    [91]	validation_0-logloss:0.51745
    [92]	validation_0-logloss:0.52117
    [93]	validation_0-logloss:0.52315
    [94]	validation_0-logloss:0.52558
    [95]	validation_0-logloss:0.52690
    [96]	validation_0-logloss:0.52668
    [97]	validation_0-logloss:0.52683
    [98]	validation_0-logloss:0.52765
    [99]	validation_0-logloss:0.52881



```python
get_clf_eval(y_test1,xgb_pred)
```

    오차 행렬
    [[96 11]
     [21 51]]
    정확도: 0.8212, 정밀도: 0.8226, 재현율: 0.7083,    F1: 0.7612, AUC:0.8028


||최적화 전|최적화 후|
|------|:---:|:---:|
|**정확도**|0.8324|0.8212
|**정밀도**|0.8281|0.8226
|**재현율**|0.7361|0.7083
|**F1 score**|0.7794|0.7612
|**AUC**|0.8167|0.8028

- GridSearcgCV로 찾은 최적 파라미터로 모델을 수행한 결과, 모델의 성능이 더 나빠졌다.

## 5. 랜덤 포레스트

### 5-1. 모든 변수로 학습


```python
from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=156)

rf_clf.fit(X_train, y_train)
rf_pred = rf_clf.predict(X_test)

get_clf_eval(y_test,rf_pred)
```

    오차 행렬
    [[95 12]
     [18 54]]
    정확도: 0.8324, 정밀도: 0.8182, 재현율: 0.7500,    F1: 0.7826, AUC:0.8189


최적화


```python
params = {
    'n_estimators':[500],
    'max_depth' : [6, 8, 10, 12], 
    'min_samples_leaf' : [8, 12, 18 ],
    'min_samples_split' : [20, 25, 30, 35]
}

rf_clf = RandomForestClassifier(random_state=11, n_jobs=-1)
grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 )
grid_cv.fit(X_train , y_train)

print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
```

    최적 하이퍼 파라미터:
     {'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 20, 'n_estimators': 500}



```python
rf_clf1 = RandomForestClassifier(n_estimators=500, max_depth=6, min_samples_leaf=8,
                                min_samples_split=20, random_state=156)
rf_clf1.fit(X_train, y_train)
pred = rf_clf1.predict(X_test)

get_clf_eval(y_test,pred)
```

    오차 행렬
    [[98  9]
     [20 52]]
    정확도: 0.8380, 정밀도: 0.8525, 재현율: 0.7222,    F1: 0.7820, AUC:0.8191


### 5-2. Parch 제외


```python
rf_clf.fit(X_train2, y_train2)
rf_pred2 = rf_clf.predict(X_test2)

get_clf_eval(y_test,rf_pred2)
```

    오차 행렬
    [[96 11]
     [19 53]]
    정확도: 0.8324, 정밀도: 0.8281, 재현율: 0.7361,    F1: 0.7794, AUC:0.8167


최적화


```python
rf_clf = RandomForestClassifier(random_state=11, n_jobs=-1)
grid_cv2 = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 )
grid_cv2.fit(X_train2 , y_train2)

print('최적 하이퍼 파라미터:\n', grid_cv2.best_params_)
```

    최적 하이퍼 파라미터:
     {'max_depth': 6, 'min_samples_leaf': 8, 'min_samples_split': 30, 'n_estimators': 500}



```python
rf_clf3 = RandomForestClassifier(n_estimators=500, max_depth=6, min_samples_leaf=8,
                                min_samples_split=20, random_state=156)
rf_clf3.fit(X_train2, y_train2)
pred2 = rf_clf3.predict(X_test2)

get_clf_eval(y_test2, pred2)   
```

    오차 행렬
    [[97 10]
     [19 53]]
    정확도: 0.8380, 정밀도: 0.8413, 재현율: 0.7361,    F1: 0.7852, AUC:0.8213

